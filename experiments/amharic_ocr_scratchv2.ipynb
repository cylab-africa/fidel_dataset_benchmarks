{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UK7J-dp5iN5",
        "outputId": "4f3eb3cc-e531-4274-c622-b5e8e951dbbb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYgaLmgy5iqR",
        "outputId": "f4041c3d-5047-4907-e62c-472bc5c099d7"
      },
      "outputs": [],
      "source": [
        "''' Installing some required libraries. '''\n",
        "!pip install python-levenshtein torchsummaryX wandb kaggle pytorch-nlp --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKnnoWbZcppL",
        "outputId": "755aabe5-a01b-4bc5-8bad-3ad631acd183"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLXQgnj7E8BM"
      },
      "source": [
        "##  Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mkii-6Dsjr8",
        "outputId": "576f6039-a2bd-49ee-bae1-e361227d0c6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from torchsummary import summary\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import math\n",
        "import yaml\n",
        "import random\n",
        "import zipfile\n",
        "import datetime\n",
        "\n",
        "import glob\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "''' Imports for decoding and distance calculation. '''\n",
        "import Levenshtein\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "import requests\n",
        "from PIL import Image\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOBPQjzrx5n"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkzzA5l2VbXZ",
        "outputId": "5ba0558a-1641-4699-e6ca-099c92445239"
      },
      "outputs": [],
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "###### Dataset -----------------------------------------------------------------\n",
        "train_dataset   : \"train-clean-100\"     # train-clean-50 (primarily for debugging purposes), train-clean-100\n",
        "cepstral_norm   : True\n",
        "input_dim       : 27\n",
        "batch_size      : 64                    # decrease this as you modify the network architecture\n",
        "\n",
        "###### Encoder Parameters ------------------------------------------\n",
        "## Universal (Part I and II)\n",
        "enc_dropout     : 0.2                   # [0.1, 0.4]\n",
        "## Transformer-related (Part II)\n",
        "enc_num_layers  : 1                     # [1, 3]\n",
        "enc_num_heads   : 1                     # [1, 4]\n",
        "\n",
        "###### Decoder Parameters ------------------------------------------\n",
        "## Transformer-related (Part I and II)\n",
        "dec_dropout     : 0.2                   # [0.1, 0.4]\n",
        "dec_num_layers  : 4                     # [1, 3]\n",
        "dec_num_heads   : 4                     # [1, 4]\n",
        "\n",
        "###### Network Parameters ------------------------------------------------------\n",
        "d_model         : 512                   # [256, 1024]\n",
        "d_ff            : 2048                  # [512, 4096]\n",
        "\n",
        "###### Learning Rate ---------------------------------------------------------------\n",
        "learning_rate   : 1E-4                  # [1E-3, 1E-4], this will depend on the specified optimizer\n",
        "\n",
        "###### Optimizer ---------------------------------------------------------------\n",
        "optimizer       : \"AdamW\"               # Adam, AdamW\n",
        "\n",
        "## if SGD\n",
        "momentum        : 0.0\n",
        "nesterov        : True\n",
        "\n",
        "###### Scheduler ---------------------------------------------------------------\n",
        "scheduler       : \"CosineAnnealing\"     # CosineAnnealing, ReduceLR\n",
        "\n",
        "## if ReduceLR\n",
        "\n",
        "## we are validating every 2 epochs but scheduler acts on every epoch. Set patience accordingly\n",
        "## patience less than validation frquency can mean learning rate always dropping after patience epochs\n",
        "## specify a suitable threshold too\n",
        "factor          : 0.9\n",
        "patience        : 6\n",
        "\n",
        "###### Training Parameters -----------------------------------------------------\n",
        "epochs          : 100\n",
        "\n",
        "###### Name --------------------------------------------------------------------\n",
        "Name: \"idl_man_4\"                                # write your name here for study group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcQEffYWYOVm"
      },
      "outputs": [],
      "source": [
        "with open(\"config.yaml\") as file:\n",
        "    config = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgkkGyEa4TMS",
        "outputId": "3ce4d11e-150f-48b1-99c1-4dfe469e339c"
      },
      "outputs": [],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlOsyc4rdEMa"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/HHD-Ethiopic/dictionary_fixed.json', encoding=\"utf-8\") as json_file:\n",
        "    raw_data = json_file.read()\n",
        "raw_data = json.loads(raw_data)\n",
        "# raw_data[\"307\"] = \"<sos>\"\n",
        "# raw_data[\"308\"] = \"<eos>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7qIBnI4gcpJ",
        "outputId": "d9c1c3c2-7397-4737-f0b9-5419b8790143"
      },
      "outputs": [],
      "source": [
        "x = np.load(\"train_numpy/x_train.npy\", allow_pickle=True)\n",
        "y = np.load(\"train_numpy/y_train.npy\", allow_pickle=True)\n",
        "# Split into 80% training and 20% validation\n",
        "x, x_val, y, y_val = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "\n",
        "print(len(x_val), len(x) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "3521f963ce2948c9a63270125659ea75",
            "e13025db21c5478183fc45a12d1ab947",
            "b4ad3a6341b84dd09e0bd185ed673c01",
            "92cde89a1b7b4b99ac17d27b33eb5587",
            "33d66a23d9534509a2f119a92b2b114a",
            "c371e8b3780b4f8294b113bb5e6b11f9",
            "0d0cddff7d2949e0b2c8f533a4315db7",
            "6ebd6b14277d4014becf95543018c89c",
            "74c625b14182431bb71d1ad45eb31003",
            "e7a3f99062d44556a752ebc542cde030",
            "159f2db1cce042c7807c0c8a86252666",
            "173e63f02bac4d8f9c9a7085cabf6f56",
            "f2edac223355466b9bd1ab170bcf3982",
            "bad661cbe92f42e3b47d91fc63ef99cf",
            "efaa2670b38f441fb703e05c6d2dd71d",
            "8044c937b23048e8b7a6e9a727f9c21c",
            "693ea5a123b843b2b6bc3962274cfce0",
            "bb17277267e0416383f08404eabf2954",
            "3039b16bbbd5434eb85a2014e836c633",
            "fe72220f07d049e992715ec9ffa79164",
            "d43cc641d66849928302c3e3ad5c9a14",
            "4231389f56a84823a0ec9239911a5bdb",
            "cec9cf42f9564aff9e4eb37b0e69c227",
            "88715eac854d46ce805914b8c79a32e0",
            "ff83fb1004124e4295989e9b92de42da",
            "6dc977a0d3c54ec1a2e60844c49146a4",
            "ed7f86f5de8b4407b26ebc7d7f631c02",
            "373c9e6af2634f72aa15ca32786748c1",
            "06715b7f0ffe4c25925bd68b6f626de7",
            "806d0f3f3ece45cd9fb82b9d62022358",
            "ff9b40e257a34cb28691128f9f8e3fbe",
            "4324d33c846f456aa43b3de5725d46a4",
            "93f1ef8ef6fd412583815998136f7ef2",
            "9a23f989d62a464aba22000906a9063e",
            "897f98d415394b3c9bc5eaed61ffcfb1",
            "71459ce2ca81455c9997eb3b6eb9a92e",
            "5c0a6316ab924e0797b9cc82bda742fe",
            "0533f79de9f049b7aa640fb6b53923d7",
            "3eb26c1a65b14a88a7399ac5c091e91a",
            "7b00fc0abae048bb88981e69728201b2",
            "8e9bc34111ef436d8abdff79bbd76f92",
            "a1c229a837fd4730918453065a823a32",
            "a413ef10196e4c3182ef963b2ad2d51a",
            "b05d8e951da94c98bd5a2e2b18c495f4",
            "178e5fc1784c443c8ab8bfe5481410c4",
            "7c48abe4289941d68d8dd49f024a4e1a",
            "6c599ea368724b02aa26ea2248d094b3",
            "f24e25523d6d4b3b861304b51913884c",
            "cb5a107a82944b3b9dc8ebb3743b735e",
            "3bdf1473bab1424896edb165a009af59",
            "7ecb7c7f48794981aa2d9f6232fa232b",
            "da71027381af42849bffb3d42a60c8c4",
            "10e4ef60303b4cbb948ab52ee84be605",
            "e6e05b1528ed404d94433e94b961d520",
            "eb0406c519a9470caed2ca05c315b692"
          ]
        },
        "id": "DfHuQmSYi389",
        "outputId": "0e9ff007-47f9-4f30-a3e2-ed4917f6ecf1"
      },
      "outputs": [],
      "source": [
        "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xACDLJdVgRDx",
        "outputId": "a3bea0f2-2a19-4caf-9c4c-8a60dc824d3b"
      },
      "outputs": [],
      "source": [
        "VOCAB = [\n",
        "    \"<pad>\", \"<sos>\", \"<eos>\",\n",
        "] + list(raw_data.values())\n",
        "\n",
        "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
        "\n",
        "PAD_TOKEN = VOCAB_MAP[\"<pad>\"]\n",
        "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
        "\n",
        "print(f\"Length of Vocabulary    : {len(VOCAB)}\")\n",
        "print(f\"VOCAB                   : {VOCAB}\")\n",
        "print(f\"PAD_TOKEN               : {PAD_TOKEN}\")\n",
        "print(f\"SOS_TOKEN               : {SOS_TOKEN}\")\n",
        "print(f\"EOS_TOKEN               : {EOS_TOKEN}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUVS88eqhttE"
      },
      "outputs": [],
      "source": [
        "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rnNlFsfdNug"
      },
      "outputs": [],
      "source": [
        "''' This class is very similar to HW3P2 except for targets (now sequence of characters). '''\n",
        "\n",
        "class AmharicOcrDataset(torch.utils.data.Dataset):\n",
        "    ''' memory inefficient : loading data in __init__ for simplicty\n",
        "\n",
        "        You may decide to load data in __getitem__ if you wish.\n",
        "        However, doing this memory inefficiently makes __init__ function takes\n",
        "        the load of loading the data, and shifts it away from during the training process.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, images_array, transcript_array, processor):\n",
        "\n",
        "\n",
        "        self.images = []\n",
        "        self.transcript = transcript_array\n",
        "\n",
        "        self.processor = processor\n",
        "\n",
        "        for image in images_array:\n",
        "          image  =cv2.resize(image/255.0, dsize=(224, 224))\n",
        "          image = np.dstack((image,image,image))\n",
        "          # pixel_values = self.processor(image, return_tensors=\"pt\",do_rescale=False).pixel_values\n",
        "\n",
        "          self.images.append(image)\n",
        "\n",
        "        self.transcripts_shifted = []\n",
        "        self.transcripts_golden = []\n",
        "\n",
        "\n",
        "\n",
        "        for temp in self.transcript:\n",
        "            # temp = np.load(file)[1:-1]\n",
        "\n",
        "            # Why do we have two different types of targets?\n",
        "            # How do we want our decoder to know the start of sequence <SOS> and end of sequence <EOS>?\n",
        "\n",
        "            # @TODO: Uncomment the code below after answering the above questions.\n",
        "            self.transcripts_shifted.append(np.array([SOS_TOKEN] + [i for i in temp if i != 0]))\n",
        "            self.transcripts_golden.append(np.array([i for i in temp if i != 0] + [EOS_TOKEN]))\n",
        "        self.length         = len(self.images)\n",
        "        assert len(self.images) == len(self.transcripts_shifted)\n",
        "\n",
        "\n",
        "    def __len__(self): return self.length\n",
        "\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        image                = torch.FloatTensor(self.images[ind])\n",
        "        shifted_transcript  = torch.tensor(self.transcripts_shifted[ind])\n",
        "        golden_transcript   = torch.tensor(self.transcripts_golden[ind])\n",
        "\n",
        "        return image, shifted_transcript, golden_transcript\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        # @NOTE: batch corresponds to output from __getitem__ for a minibatch\n",
        "\n",
        "        '''\n",
        "        1.  Extract the features and labels from 'batch'.\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at PyTorch's documentation for pad_sequence.\n",
        "        3.  This is a good place to perform transforms, if you so wish.\n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lengths of features, and lengths of labels.\n",
        "\n",
        "        '''\n",
        "\n",
        "        batch_images              = [self.processor( i[0] , return_tensors=\"pt\",do_rescale=False).pixel_values for i in batch]\n",
        "\n",
        "        # Batch of output characters (shifted and golden).\n",
        "        batch_transcript        = [i[1] for i in batch]\n",
        "        batch_golden            = [i[2] for i in batch]\n",
        "\n",
        "        lengths_mfcc            = [len(i) for i in batch_images]\n",
        "        lengths_transcript      = [len(i) for i in batch_transcript]\n",
        "\n",
        "\n",
        "        batch_images_pad          = pad_sequence(batch_images, batch_first=True, padding_value=PAD_TOKEN)\n",
        "        batch_transcript_pad    = pad_sequence(batch_transcript, batch_first=True, padding_value=PAD_TOKEN)\n",
        "        batch_golden_pad        = pad_sequence(batch_golden, batch_first=True, padding_value=PAD_TOKEN)\n",
        "\n",
        "\n",
        "        # You may apply some transformations, Time and Frequency masking, here in the collate function:\n",
        "        # Food for Thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "\n",
        "\n",
        "        # Return the following values:\n",
        "        # padded features, padded shifted labels, padded golden labels, actual length of features, actual length of the shifted labels\n",
        "        return batch_images_pad, batch_transcript_pad, batch_golden_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI-EyabpDPT1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class AmharicOcrDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_array, transcript_array, processor):\n",
        "        self.images_paths = images_array  # Store only paths, not images\n",
        "        self.transcript = transcript_array\n",
        "        self.processor = processor\n",
        "        self.length = len(images_array)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        # Load image only when needed\n",
        "        image = self.images_paths[ind]\n",
        "        image = cv2.resize(image / 255.0, (224, 224))\n",
        "        image = np.dstack((image, image, image))  # Convert to 3-channel\n",
        "        image = torch.FloatTensor(image)\n",
        "\n",
        "        shifted_transcript = torch.tensor([SOS_TOKEN] + [i for i in self.transcript[ind] if i != 0])\n",
        "        golden_transcript = torch.tensor([i for i in self.transcript[ind] if i != 0] + [EOS_TOKEN])\n",
        "\n",
        "        return image, shifted_transcript, golden_transcript\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        batch_images = [self.processor(i[0], return_tensors=\"pt\", do_rescale=False).pixel_values for i in batch]\n",
        "        batch_transcript = [i[1] for i in batch]\n",
        "        batch_golden = [i[2] for i in batch]\n",
        "\n",
        "        lengths_mfcc = [len(i) for i in batch_images]\n",
        "        lengths_transcript = [len(i) for i in batch_transcript]\n",
        "\n",
        "        batch_images_pad = pad_sequence(batch_images, batch_first=True, padding_value=PAD_TOKEN)\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True, padding_value=PAD_TOKEN)\n",
        "        batch_golden_pad = pad_sequence(batch_golden, batch_first=True, padding_value=PAD_TOKEN)\n",
        "\n",
        "        return batch_images_pad, batch_transcript_pad, batch_golden_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpY8bqq3ckzQ",
        "outputId": "be9d8ea0-0c43-474a-be37-8a603009aaf8"
      },
      "outputs": [],
      "source": [
        "train_dataset   = AmharicOcrDataset(\n",
        "x,y,processor\n",
        ")\n",
        "\n",
        "val_dataset   = AmharicOcrDataset(\n",
        "x_val,y_val,processor\n",
        ")\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5GiDlTCd4G8",
        "outputId": "44d6449f-3d72-4d4a-979c-1a85878e4cd7"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader    = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset,\n",
        "    batch_size  = config[\"batch_size\"],\n",
        "    shuffle     = True,\n",
        "    num_workers = 4,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = train_dataset.collate_fn\n",
        ")\n",
        "\n",
        "val_loader      = torch.utils.data.DataLoader(\n",
        "    dataset     = val_dataset,\n",
        "    batch_size  = config[\"batch_size\"],\n",
        "    shuffle     = False,\n",
        "    num_workers = 2,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = val_dataset.collate_fn,\n",
        ")\n",
        "\n",
        "print(\"No. of Train Images   : \", train_dataset.__len__())\n",
        "print(\"Batch Size           : \", config[\"batch_size\"])\n",
        "print(\"Train Batches        : \", train_loader.__len__())\n",
        "print(\"Val Batches          : \", val_loader.__len__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8fNzxginJRn",
        "outputId": "fc129fd2-cef7-4328-af6f-910d224b1c25"
      },
      "outputs": [],
      "source": [
        "''' Sanity Check '''\n",
        "\n",
        "print(\"Checking the Shapes of the Data --\\n\")\n",
        "\n",
        "for batch in train_loader:\n",
        "    x_pad, y_shifted_pad, y_golden_pad, x_len, y_len, = batch\n",
        "\n",
        "    print(f\"x_pad shape:\\t\\t{x_pad.shape}\")\n",
        "    print(f\"x_len shape:\\t\\t{x_len.shape}\\n\")\n",
        "\n",
        "    print(f\"y_shifted_pad shape:\\t{y_shifted_pad.shape}\")\n",
        "    print(f\"y_golden_pad shape:\\t{y_golden_pad.shape}\")\n",
        "    print(f\"y_len shape:\\t\\t{y_len.shape}\\n\")\n",
        "\n",
        "    # print(y_shifted_pad)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCty-GBJn69r"
      },
      "outputs": [],
      "source": [
        "def create_mask_1(padded_input, input_lengths=None, pad_idx=None):\n",
        "    \"\"\" Create a mask to identify non-padding positions.\n",
        "\n",
        "    Args:\n",
        "        padded_input: The input tensor with padding, shape (N, T, ...) or (N, T).\n",
        "        input_lengths: Optional, the actual lengths of each sequence before padding, shape (N,).\n",
        "        pad_idx: Optional, the index used for padding tokens.\n",
        "\n",
        "    Returns:\n",
        "        A mask tensor with shape (N, T, 1), where non-padding positions are marked with 1 and padding positions are marked with 0.\n",
        "    \"\"\"\n",
        "\n",
        "    assert input_lengths is not None or pad_idx is not None\n",
        "\n",
        "    # Create a mask based on input_lengths\n",
        "    if input_lengths is not None:\n",
        "        N = padded_input.size(0)        # padded_input : (N x T x ...)\n",
        "        non_pad_mask = padded_input.new_ones(padded_input.size()[:-1])  # (N x T)\n",
        "\n",
        "        # Set the mask to 0 for padding positions\n",
        "        for i in range(N):\n",
        "          non_pad_mask[i, input_lengths[i]:] = 0\n",
        "\n",
        "    if pad_idx is not None:             # padded_input : N x T\n",
        "\n",
        "        assert padded_input.dim() == 2\n",
        "\n",
        "        # Create a mask where non-padding positions are marked with 1 and padding positions are marked with 0\n",
        "        non_pad_mask = padded_input.ne(pad_idx).float()\n",
        "\n",
        "    return non_pad_mask.unsqueeze(-1)   # unsqueeze(-1) for broadcasting\n",
        "\n",
        "def create_mask_2(seq, pad_idx=None):\n",
        "    \"\"\" Create a mask to prevent positions from attending to subsequent positions.\n",
        "\n",
        "    Args:\n",
        "        seq: The input sequence tensor, shape (batch_size, sequence_length).\n",
        "\n",
        "    Returns:\n",
        "        A mask tensor with shape (batch_size, sequence_length, sequence_length),\n",
        "            where positions are allowed to attend to previous positions but not to subsequent positions.\n",
        "    \"\"\"\n",
        "\n",
        "    sz_b, len_s = seq.size()\n",
        "\n",
        "    # Create an upper triangular matrix with zeros on the diagonal and below (indicating allowed positions)\n",
        "    #   and ones above the diagonal (indicating disallowed positions)\n",
        "    subsequent_mask = torch.triu(\n",
        "        torch.ones((len_s, len_s), device=seq.device, dtype=torch.uint8), diagonal=1)\n",
        "\n",
        "    # Expand the mask to match the batch size, resulting in a mask for each sequence in the batch.\n",
        "    mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n",
        "\n",
        "\n",
        "    ''' Create a mask to ignore padding positions in the key sequence during attention calculation. '''\n",
        "\n",
        "    # Expanding to fit the shape of key query attention matrix.\n",
        "    if pad_idx != None:\n",
        "      len_q = seq.size(1)\n",
        "\n",
        "      # Create a mask where padding positions in the key sequence are marked with 1.\n",
        "      padding_mask  = seq.eq(pad_idx)\n",
        "\n",
        "      # Expand the mask to match the dimensions of the key-query attention matrix.\n",
        "      padding_mask  = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
        "      mask          = (padding_mask + mask).gt(0)\n",
        "\n",
        "    else:\n",
        "      mask = mask.gt(0)\n",
        "\n",
        "    return mask\n",
        "\n",
        "def create_mask_3(padded_input, input_lengths, expand_length):\n",
        "    \"\"\" Create an attention mask to ignore padding positions in the input sequence during attention calculation.\n",
        "\n",
        "    Args:\n",
        "        padded_input: The input tensor with padding, shape (N, Ti, ...).\n",
        "        input_lengths: The actual lengths of each sequence before padding, shape (N,).\n",
        "        expand_length: The length to which the attention mask should be expanded,\n",
        "            usually equal to the length of the sequence that the attention scores will be applied to.\n",
        "\n",
        "    Returns:\n",
        "        An attention mask tensor with shape (N, expand_length, Ti),\n",
        "            where padding positions in the input sequence are marked with 1 and other positions are marked with 0.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a mask to identify non-padding positions, shape (N, Ti, 1)\n",
        "    # (N x Ti x 1)\n",
        "    non_pad_mask    = create_mask_1(padded_input, input_lengths=input_lengths)\n",
        "\n",
        "    # Invert the mask to identify padding positions, shape (N, Ti)\n",
        "    # N x Ti, lt(1) like-not operation\n",
        "    pad_mask        = non_pad_mask.squeeze(-1).lt(1)\n",
        "\n",
        "\n",
        "    # Expand the mask to match the dimensions of the attention matrix, shape (N, expand_length, Ti)\n",
        "    attn_mask       = pad_mask.unsqueeze(1).expand(-1, expand_length, -1)\n",
        "\n",
        "    return attn_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3kCAnw-oBJt"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(torch.nn.Module):\n",
        "    ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature    = temperature                       # Scaling factor for the dot product\n",
        "        self.dropout        = torch.nn.Dropout(attn_dropout)    # Dropout layer for attention weights\n",
        "        self.softmax        = torch.nn.Softmax(dim=2)           # Softmax layer along the attention dimension\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        # Calculate the dot product between queries and keys.\n",
        "        attn = torch.bmm(q, k.transpose(1, 2))\n",
        "\n",
        "        # Scale the dot product by the temperature.\n",
        "        attn = attn / self.temperature\n",
        "\n",
        "        if mask is not None:\n",
        "            # Apply the mask by setting masked positions to a large negative value.\n",
        "            # This ensures they have a softmax score close to zero.\n",
        "            mask_value = -1e+30 if attn.dtype == torch.float32 else -1e+4\n",
        "            attn = attn.masked_fill(mask, mask_value)\n",
        "\n",
        "        # Apply softmax to obtain attention weights.\n",
        "        attn    = self.softmax(attn)\n",
        "\n",
        "        # Apply dropout to the attention weights.\n",
        "        attn    = self.dropout(attn)\n",
        "\n",
        "        # Compute the weighted sum of values based on the attention weights.\n",
        "        output  = torch.bmm(attn, v)\n",
        "\n",
        "        return output, attn # Return the attention output and the attention weights.\n",
        "def save_attention_plot(attention_weights, epoch=0):\n",
        "    ''' function for saving attention weights plot to a file\n",
        "\n",
        "        @NOTE: default starter code set to save cross attention\n",
        "    '''\n",
        "\n",
        "    plt.clf()  # Clear the current figure\n",
        "    sns.heatmap(attention_weights, cmap=\"GnBu\")  # Create heatmap\n",
        "\n",
        "    # Save the plot to a file. Specify the directory if needed.\n",
        "    plt.savefig(f\"cross_attention-epoch{epoch}.png\")\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    ''' Multi-Head Attention Module '''\n",
        "\n",
        "    def __init__(self, n_head, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head # Number of attention heads\n",
        "        self.d_k    = d_model // n_head\n",
        "        self.d_v    = d_model // n_head\n",
        "\n",
        "\n",
        "        # Linear layers for projecting the input query, key, and value to multiple heads\n",
        "        self.w_qs   = torch.nn.Linear(d_model, n_head * self.d_k)\n",
        "        self.w_ks   = torch.nn.Linear(d_model, n_head * self.d_k)\n",
        "        self.w_vs   = torch.nn.Linear(d_model, n_head * self.d_v)\n",
        "\n",
        "        torch.nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_k)))\n",
        "        torch.nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_k)))\n",
        "        torch.nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_v)))\n",
        "\n",
        "        # Initialize the weights of the linear layers\n",
        "        self.attention = ScaledDotProductAttention(\n",
        "            temperature=np.power(self.d_k, 0.5), attn_dropout=dropout)\n",
        "\n",
        "        # Final linear layer to project the concatenated outputs of the attention heads back to the model dimension\n",
        "        self.fc = torch.nn.Linear(n_head * self.d_v, d_model)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.fc.weight)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        # following key, value, query standard computation\n",
        "        d_k, d_v, n_head    = self.d_k, self.d_v, self.n_head\n",
        "        sz_b, len_q, _      = q.size()\n",
        "        sz_b, len_k, _      = k.size()\n",
        "        sz_b, len_v, _      = v.size()\n",
        "\n",
        "        # Project the input query, key, and value to multiple heads\n",
        "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
        "\n",
        "        # Rearrange the dimensions to group the heads together for parallel processing\n",
        "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk\n",
        "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk\n",
        "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv\n",
        "\n",
        "        # Repeat the mask for each attention head if a mask is provided\n",
        "        if mask is not None:\n",
        "            mask = mask.repeat(n_head, 1, 1)\n",
        "\n",
        "        # Apply scaled dot-product attention to the projected query, key, and value\n",
        "        output, attn    = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # Rearrange the output back to the original order and concatenate the heads\n",
        "        output          = output.view(n_head, sz_b, len_q, d_v)\n",
        "        output          = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dv)\n",
        "\n",
        "        output          = self.dropout(self.fc(output))\n",
        "\n",
        "        return output, attn\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    ''' Position Encoding from Attention Is All You Need Paper '''\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize a tensor to hold the positional encodings\n",
        "        pe          = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Create a tensor representing the positions (0 to max_len-1)\n",
        "        position    = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Calculate the division term for the sine and cosine functions\n",
        "        # This term creates a series of values that decrease geometrically, used to generate varying frequencies for positional encodings\n",
        "        div_term    = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Compute the positional encodings using sine and cosine functions\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Reshape the positional encodings tensor and make it a buffer\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      return x + self.pe[:, :x.size(1)]\n",
        "class FeedForward(torch.nn.Module):\n",
        "    ''' Projection Layer (Fully Connected Layers) '''\n",
        "\n",
        "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1   = torch.nn.Linear(d_model, d_ff)\n",
        "        self.dropout    = torch.nn.Dropout(dropout)\n",
        "        self.linear_2   = torch.nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Apply the first linear layer, GeLU activation, and then dropout\n",
        "        x = self.dropout(torch.nn.functional.gelu(self.linear_1(x)))\n",
        "\n",
        "         # Apply the second linear layer to project the dimension back to d_model\n",
        "        x = self.linear_2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEz7HkOJoDjD"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # @TODO: fill in the blanks appropriately (given the modules above)\n",
        "        self.mha1       = MultiHeadAttention(num_heads, d_model, dropout=dropout)\n",
        "        self.mha2       = MultiHeadAttention(num_heads, d_model, dropout=dropout)\n",
        "        self.ffn        = FeedForward(d_model, d_ff)\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1   = nn.Dropout(p=dropout)\n",
        "        self.dropout2   = nn.Dropout(p=dropout)\n",
        "        self.dropout3   = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, padded_targets, enc_output, enc_input_lengths, dec_enc_attn_mask, pad_mask, slf_attn_mask):\n",
        "\n",
        "        output1, attn_weights1 = self.mha1(padded_targets, padded_targets, padded_targets, slf_attn_mask )\n",
        "\n",
        "        # Skip (Residual) Connections\n",
        "        #   (1) perform dropout on padded attention output\n",
        "        output1 = self.dropout1(output1)\n",
        "        #   (2) add the true outputs (padded_targets) as a skip connection\n",
        "\n",
        "#         ''' TODO '''\n",
        "\n",
        "        # Layer Normalization\n",
        "        #   (1) call layernorm on this resulting value\n",
        "        output1 = self.layernorm1(output1 +  padded_targets)\n",
        "\n",
        "#         ''' TODO '''\n",
        "\n",
        "        # Masked Multi-Head Attention on Encoder Outputs and Targets\n",
        "        #   (1) apply MHA with the self-attention mask\n",
        "        #forward(self, q, k, v, mask=None)\n",
        "\n",
        "\n",
        "        output2, attn_weights2 = self.mha2(output1, enc_output, enc_output, dec_enc_attn_mask )\n",
        "\n",
        "\n",
        "#         ''' TODO '''\n",
        "\n",
        "        # Skip (Residual) Connections\n",
        "        #   (1) perform dropout on this second padded attention output\n",
        "        output2 = self.dropout2(output2)\n",
        "\n",
        "        #   (2) add the output of first MHA block as a skip connection\n",
        "\n",
        "#         ''' TODO '''\n",
        "\n",
        "        # Layer Normalization\n",
        "        #   (1) call layernorm on this resulting value\n",
        "\n",
        "        output2 = self.layernorm2(output2 + output1)\n",
        "\n",
        "#         ''' TODO '''\n",
        "\n",
        "        # Feed Forward Network\n",
        "        #   (1) pass through the FFN\n",
        "        ffn_out = self.ffn(output2)\n",
        "#         ''' TODO '''\n",
        "\n",
        "        # Skip (Residual) Connections\n",
        "        #   (1) perform dropout on the output\n",
        "        ffn_out = self.dropout3(ffn_out)\n",
        "\n",
        "\n",
        "        #   (2) add the output of second MHA block as a skip connection\n",
        "\n",
        "        #''' TODO '''\n",
        "\n",
        "        # apply Layer Normalization onto output of feed-forward network\n",
        "        output3 = self.layernorm3(ffn_out + output2)\n",
        "        return output3, attn_weights1, attn_weights2\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout,\n",
        "            target_vocab_size, max_seq_length, eos_token, sos_token, pad_token):\n",
        "        super().__init__()\n",
        "\n",
        "        self.EOS_TOKEN      = eos_token\n",
        "        self.SOS_TOKEN      = sos_token\n",
        "        self.PAD_TOKEN      = pad_token\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_layers     = num_layers\n",
        "\n",
        "        # use torch.nn.ModuleList() with list comprehension looping through num_layers\n",
        "        # @NOTE: think about what stays constant per each DecoderLayer (how to call DecoderLayer)\n",
        "        # @HINT: We've implemented this for you.\n",
        "        self.dec_layers = torch.nn.ModuleList(\n",
        "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.target_embedding       = nn.Embedding(target_vocab_size, d_model)\n",
        "        self.positional_encoding    = PositionalEncoding(d_model, max_len=max_seq_length)\n",
        "        self.final_linear           = nn.Linear(d_model, target_vocab_size)\n",
        "        self.dropout                = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, padded_targets, enc_output, enc_input_lengths):\n",
        "\n",
        "        pad_mask = create_mask_1(padded_targets,  pad_idx= self.PAD_TOKEN)\n",
        "\n",
        "        look_ahead_mask = create_mask_2(padded_targets,  pad_idx= self.PAD_TOKEN) ##\n",
        "\n",
        "        dec_enc_attn_mask = create_mask_3(enc_output, enc_input_lengths,padded_targets.shape[1])\n",
        "\n",
        "        target_embedded = self.target_embedding(padded_targets)\n",
        "\n",
        "\n",
        "        target_embedded = self.positional_encoding(target_embedded)\n",
        "\n",
        "        target_embedded = self.dropout(target_embedded)\n",
        "\n",
        "        running_attn = {}\n",
        "\n",
        "\n",
        "        for i, layer in enumerate(self.dec_layers):\n",
        "            target_embedded, attn1, attn = layer(target_embedded, enc_output, enc_input_lengths, dec_enc_attn_mask, pad_mask, look_ahead_mask)\n",
        "            running_attn['layer{}_dec_self'.format(i + 1)] = attn1\n",
        "            running_attn['layer{}_dec_self'.format(i + 1)] =  attn\n",
        "\n",
        "\n",
        "        # target_embedded = self.layer_norm(target_embedded)\n",
        "        out = self.final_linear(target_embedded)\n",
        "\n",
        "\n",
        "        return out, running_attn\n",
        "        raise NotImplemented\n",
        "\n",
        "\n",
        "    def recognize_greedy_search(self, enc_outputs, enc_input_lengths):\n",
        "        ''' passes the encoder outputs and its corresponding lengths through autoregressive network\n",
        "\n",
        "            @NOTE: You do not need to make changes to this method.\n",
        "        '''\n",
        "\n",
        "        batch_size = enc_outputs.size(0)\n",
        "\n",
        "        # start with the <SOS> token for each sequence in the batch\n",
        "        target_seq = torch.full((batch_size, 1), self.SOS_TOKEN, dtype=torch.long).to(enc_outputs.device)\n",
        "\n",
        "        finished = torch.zeros(batch_size, dtype=torch.bool).to(enc_outputs.device)\n",
        "\n",
        "        for _ in range(self.max_seq_length):\n",
        "\n",
        "            # preparing attention masks\n",
        "            # filled with ones becaues we want to attend to all the elements in the sequence\n",
        "            pad_mask = torch.ones_like(target_seq).float().unsqueeze(-1)  # (batch_size x i x 1)\n",
        "            slf_attn_mask_subseq = create_mask_2(target_seq)\n",
        "\n",
        "            x = self.positional_encoding(self.target_embedding(target_seq))\n",
        "\n",
        "            for i in range(self.num_layers):\n",
        "                x, block1, block2 = self.dec_layers[i](\n",
        "                    x, enc_outputs, enc_input_lengths, None, pad_mask, slf_attn_mask_subseq)\n",
        "\n",
        "            seq_out = self.final_linear(x[:, -1])\n",
        "            logits = torch.nn.functional.log_softmax(seq_out, dim=1)\n",
        "\n",
        "            # selecting the token with the highest probability\n",
        "            # @NOTE: this is the autoregressive nature of the network!\n",
        "            next_token = logits.argmax(dim=-1).unsqueeze(1)\n",
        "\n",
        "            # appending the token to the sequence\n",
        "            target_seq = torch.cat([target_seq, next_token], dim=-1)\n",
        "\n",
        "            # checking if <EOS> token is generated\n",
        "            eos_mask = next_token.squeeze(-1) == self.EOS_TOKEN\n",
        "            # or opration, if both or one of them is true store the value of the finished sequence in finished variable\n",
        "            finished |= eos_mask\n",
        "\n",
        "            # end if all sequences have generated the EOS token\n",
        "            if finished.all(): break\n",
        "\n",
        "        # remove the initial <SOS> token and pad sequences to the same length\n",
        "        target_seq = target_seq[:, 1:]\n",
        "        max_length = target_seq.size(1)\n",
        "        target_seq = torch.nn.functional.pad(target_seq,\n",
        "            (0, self.max_seq_length - max_length), value=self.PAD_TOKEN)\n",
        "\n",
        "        return target_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqiAstL22fBV"
      },
      "outputs": [],
      "source": [
        "class SpeechTransformer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, dec_num_layers, dec_num_heads,\n",
        "                    d_model, d_ff, target_vocab_size, eos_token, sos_token,\n",
        "                    pad_token, enc_dropout, dec_dropout, max_seq_length=512):\n",
        "\n",
        "        super(SpeechTransformer, self).__init__()\n",
        "\n",
        "        # self.encoder    = CNN_LSTM_Encoder(input_dim, 256, enc_dropout)\n",
        "        # Load TrOCR model (encoder only)\n",
        "        self.processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "        self.encoder = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").encoder\n",
        "\n",
        "        # Extract `d_model` from TrOCR encoder\n",
        "        enc_d_model = self.encoder.config.hidden_size\n",
        "        self.proj       = torch.nn.Linear(enc_d_model, d_model)\n",
        "\n",
        "        # @NOTE: layernorm here is meant to normalize the dynamic ranges of\n",
        "        #   the cross attention weights such that the model doesn't bias\n",
        "        #   itself to only using the cross attention weights and neglect using\n",
        "        #   the multi-head self attention weights.\n",
        "        #   This is to enfoce the Language Model constraints in the Decoder.\n",
        "        #   This is an experimental change -- you may try it if you want.\n",
        "        self.layernorm  = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "        self.decoder    = Decoder(dec_num_layers, d_model, dec_num_heads, d_ff,\n",
        "                dec_dropout, target_vocab_size, max_seq_length, eos_token, sos_token, pad_token)\n",
        "\n",
        "        # You can experiment with different weight initialization schemes or no initialization here\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                torch.nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, padded_input, input_lengths, padded_target, target_lengths):\n",
        "        padded_input = padded_input.squeeze(1)\n",
        "        # passing through Encoder\n",
        "        # encoder_output, encoder_lens = self.encoder(padded_input)\n",
        "        # with torch.inference_mode():\n",
        "        encoder_output = self.encoder(padded_input)[\"last_hidden_state\"]  # Shape: (batch_size, num_patches+1, d_model)\n",
        "\n",
        "        encoder_lens = torch.tensor([encoder_output.shape[1]] * encoder_output.shape[0] )\n",
        "\n",
        "        # adding projection layer to change dimension of encoder_outputs to match d_model\n",
        "        encoder_output = self.proj(encoder_output)\n",
        "\n",
        "        # @NOTE: adding layernorm to adjust dynamic ranges of the encoder_outputs\n",
        "        # @NOTE: uncomment this if you choose to use layernorm described in __init__ above\n",
        "        encoder_output = self.layernorm(encoder_output)\n",
        "\n",
        "        # passing Encoder output and Attention masks through Decoder\n",
        "        output, attention_weights = self.decoder(padded_target, encoder_output, encoder_lens)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def recognize(self, inp, inp_len):\n",
        "        \"\"\" sequence-to-sequence greedy search -- decoding one utterence at a time \"\"\"\n",
        "\n",
        "        # encoder_outputs, encoder_lens  = self.encoder(inp, inp_len)\n",
        "        inp = inp.squeeze(1)\n",
        "        # passing through Encoder\n",
        "        # encoder_output, encoder_lens = self.encoder(padded_input)\n",
        "        with torch.inference_mode():\n",
        "          encoder_output = self.encoder(inp)[\"last_hidden_state\"]  # Shape: (batch_size, num_patches+1, d_model)\n",
        "\n",
        "        encoder_lens = torch.tensor([encoder_output.shape[1]] * encoder_output.shape[0] )\n",
        "        encoder_output                = self.proj(encoder_output)\n",
        "        out                            = self.decoder.recognize_greedy_search(encoder_output, encoder_lens)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPZD3vqdUisj"
      },
      "source": [
        "## Model Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2uX2P9YVnbk",
        "outputId": "d597477e-bedc-4eba-e482-7729ecbf1661"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9LN0l5VUk_s",
        "outputId": "1c23bc51-441b-4268-a82d-fa6727441e8d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "''' Please refer to the config file and top sections to fill in the following '''\n",
        "\n",
        "model = SpeechTransformer(\n",
        "input_dim                   = config[\"input_dim\"],\n",
        "dec_num_layers              = config[\"dec_num_layers\"],\n",
        "dec_num_heads               = config[\"dec_num_heads\"],\n",
        "\n",
        "d_model                     = config[\"d_model\"],\n",
        "d_ff                        = config[\"d_ff\"],\n",
        "\n",
        "target_vocab_size           = len(VOCAB),\n",
        "eos_token                   = EOS_TOKEN,\n",
        "sos_token                   = SOS_TOKEN,\n",
        "pad_token                   = PAD_TOKEN,\n",
        "\n",
        "enc_dropout                 = config[\"enc_dropout\"],\n",
        "dec_dropout                 = config[\"enc_dropout\"],\n",
        "\n",
        "# decrease to a small number if you are just trying to implement the network\n",
        "max_seq_length              = 48 , # Max sequence length for transcripts. Check data verification.\n",
        ").to(device)\n",
        "\n",
        "def num_parameters(mode):\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params / 1E6\n",
        "\n",
        "para = num_parameters(model)\n",
        "print(\"#\"*10)\n",
        "print(f\"Model Parameters:\\n {para}\")\n",
        "print(\"#\"*10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23DMfXsaU6kj"
      },
      "source": [
        "## Loss, Optimizer, and Scheduler Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGxaiyESLpwI"
      },
      "outputs": [],
      "source": [
        "loss_func   = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN)\n",
        "scaler      = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-jth-6HLPLr"
      },
      "outputs": [],
      "source": [
        "''' defining optimizer '''\n",
        "\n",
        "if config[\"optimizer\"] == \"SGD\":\n",
        "  # feel free to change any of the initializations you like to fit your needs\n",
        "  optimizer = torch.optim.SGD(model.parameters(),\n",
        "                              lr=config[\"learning_rate\"],\n",
        "                              momentum=config[\"momentum\"],\n",
        "                              weight_decay=1E-4,\n",
        "                              nesterov=config[\"nesterov\"])\n",
        "\n",
        "elif config[\"optimizer\"] == \"Adam\":\n",
        "  # feel free to change any of the initializations you like to fit your needs\n",
        "  optimizer = torch.optim.Adam(model.parameters(),\n",
        "                               lr=float(config[\"learning_rate\"]),\n",
        "                               weight_decay=1e-4)\n",
        "\n",
        "elif config[\"optimizer\"] == \"AdamW\":\n",
        "  # feel free to change any of the initializations you like to fit your needs\n",
        "  optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                                lr=float(config[\"learning_rate\"]),\n",
        "                                weight_decay=0.01)\n",
        "\n",
        "''' defining scheduler '''\n",
        "\n",
        "if config[\"scheduler\"] == \"ReduceLR\":\n",
        "  #Feel Free to change any of the initializations you like to fit your needs\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                factor=config[\"factor\"], patience=config[\"patience\"], min_lr=1E-8, verbose=True)\n",
        "\n",
        "elif config[\"scheduler\"] == \"CosineAnnealing\":\n",
        "  #Feel Free to change any of the initializations you like to fit your needs\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                T_max = 35, eta_min=1E-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "216ukmHbU-ol"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {\"model_state_dict\"         : model.state_dict(),\n",
        "         \"optimizer_state_dict\"     : optimizer.state_dict(),\n",
        "         \"scheduler_state_dict\"     : scheduler.state_dict() if scheduler is not None else {},\n",
        "         metric[0]                  : metric[1],\n",
        "         \"epoch\"                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= \"valid_acc\", optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "\n",
        "    epoch   = checkpoint[\"epoch\"]\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWUHzWOEs_ky"
      },
      "source": [
        "## Training and Validation Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWWO5vM4_RGC"
      },
      "source": [
        "### Levenshtein Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXNHNbnh5w5z"
      },
      "outputs": [],
      "source": [
        "''' utility function which takes a sequence of indices and converts them to a list of characters '''\n",
        "def indices_to_chars(indices, vocab):\n",
        "    tokens = []\n",
        "    for i in indices:   # looping through all indices\n",
        "\n",
        "        if int(i) == SOS_TOKEN:     # If SOS is encountered, don't add it to the final list\n",
        "            continue\n",
        "        elif int(i) == EOS_TOKEN:   # If EOS is encountered, stop the decoding process\n",
        "            break\n",
        "        else:\n",
        "            tokens.append(vocab[i])\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbv5sJSIFB8b"
      },
      "outputs": [],
      "source": [
        "''' utility function for Levenshtein Distantce quantification '''\n",
        "def calc_edit_distance(predictions, y, y_len, vocab=VOCAB, print_example=False):\n",
        "\n",
        "    dist = 0.0\n",
        "    batch_size, seq_len = predictions.shape\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "\n",
        "        y_sliced    = indices_to_chars(y[batch_idx, 0 : y_len[batch_idx]], vocab)\n",
        "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
        "\n",
        "        # strings - when you are using characters from the SpeechDataset\n",
        "        y_string    = \"\".join(y_sliced)\n",
        "        pred_string = \"\".join(pred_sliced)\n",
        "\n",
        "        dist        += Levenshtein.distance(pred_string, y_string)\n",
        "\n",
        "    if print_example:\n",
        "        print(\"\\nGround Truth : \", y_string)\n",
        "        print(\"Prediction   : \", pred_string)\n",
        "\n",
        "    dist /= batch_size\n",
        "\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0RXXoJk_WU5"
      },
      "source": [
        "### Training and Validation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJHSLMCmp8l5"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc=\"Train\")\n",
        "\n",
        "    total_loss          = 0\n",
        "    running_loss        = 0.0\n",
        "    running_perplexity  = 0.0\n",
        "\n",
        "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(train_loader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs          = inputs.to(device)\n",
        "        targets_shifted = targets_shifted.to(device)\n",
        "        targets_golden  = targets_golden.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # passing the minibatch through the model\n",
        "            # raw_predictions, attention_weights = model(inputs, inputs_lengths, targets_shifted, targets_lengths)\n",
        "            raw_predictions, attention_weights = model(inputs, inputs_lengths, targets_shifted, targets_lengths)\n",
        "\n",
        "\n",
        "            padding_mask = torch.logical_not(torch.eq(targets_shifted, PAD_TOKEN))\n",
        "\n",
        "            # cast the mask to float32\n",
        "            padding_mask = padding_mask.float()\n",
        "            loss = loss_func(raw_predictions.transpose(1,2), targets_golden)*padding_mask\n",
        "            loss = loss.sum() / padding_mask.sum()\n",
        "\n",
        "        scaler.scale(loss).backward()   # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer)          # This is a replacement for optimizer.step()\n",
        "        scaler.update()                 # This is something added just for FP16\n",
        "\n",
        "        running_loss        += float(loss.item())\n",
        "        perplexity          = torch.exp(loss)\n",
        "        running_perplexity  += perplexity.item()\n",
        "\n",
        "        # online training monitoring\n",
        "        batch_bar.set_postfix(\n",
        "            loss = \"{:.04f}\".format(float(running_loss / (i + 1))),\n",
        "            perplexity = \"{:.04f}\".format(float(running_perplexity / (i + 1)))\n",
        "        )\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    running_loss        = float(running_loss / len(train_loader))\n",
        "    running_perplexity  = float(running_perplexity / len(train_loader))\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    return running_loss, running_perplexity, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuYsBMoQ30US"
      },
      "outputs": [],
      "source": [
        "def validate_fast(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    # progress bar\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc=\"Val\", ncols=5)\n",
        "\n",
        "    running_distance = 0.0\n",
        "\n",
        "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(dataloader):\n",
        "\n",
        "        inputs  = inputs.to(device)\n",
        "        targets_golden = targets_golden.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            greedy_predictions = model.recognize(inputs, inputs_lengths)\n",
        "\n",
        "        # calculating Levenshtein Distance\n",
        "        # @NOTE: modify the print_example to print more or less validation examples\n",
        "        running_distance += calc_edit_distance(greedy_predictions, targets_golden, targets_lengths, VOCAB, print_example=True)\n",
        "\n",
        "        # online validation distance monitoring\n",
        "        batch_bar.set_postfix(\n",
        "            running_distance = \"{:.04f}\".format(float(running_distance / (i + 1)))\n",
        "        )\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if i==4: break      # validating only upon first five batches\n",
        "\n",
        "    batch_bar.close()\n",
        "    running_distance /= 5\n",
        "\n",
        "    return running_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fvRunxh_36z"
      },
      "outputs": [],
      "source": [
        "def validate_full(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    # progress bar\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc=\"Val\", ncols=5)\n",
        "\n",
        "    running_distance = 0.0\n",
        "\n",
        "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(dataloader):\n",
        "\n",
        "        inputs  = inputs.to(device)\n",
        "        targets_golden = targets_golden.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            greedy_predictions = model.recognize(inputs, inputs_lengths)\n",
        "\n",
        "        # calculating Levenshtein Distance\n",
        "        # @NOTE: modify the print_example to print more or less validation examples\n",
        "        running_distance += calc_edit_distance(greedy_predictions, targets_golden, targets_lengths, VOCAB, print_example=True)\n",
        "\n",
        "        # online validation distance monitoring\n",
        "        batch_bar.set_postfix(\n",
        "            running_distance = \"{:.04f}\".format(float(running_distance / (i + 1)))\n",
        "        )\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    batch_bar.close()\n",
        "    running_distance /= len(dataloader)\n",
        "\n",
        "    return running_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP7QK6rD_mT9"
      },
      "source": [
        "## WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "CUz-15ws79RI",
        "outputId": "db629559-e913-45ae-b62b-3d91b0bc0a81"
      },
      "outputs": [],
      "source": [
        "# using WandB? resume training?\n",
        "\n",
        "USE_WANDB = True\n",
        "RESUME_LOGGING = False\n",
        "\n",
        "# creating your WandB run\n",
        "run_name = \"{}_Transformer_ENC-{}/{}_DEC-{}/{}_{}_{}_{}_{}\".format(\n",
        "    config[\"Name\"],\n",
        "    config[\"enc_num_layers\"],       # only used in Part II with the Transformer Encoder\n",
        "    config[\"enc_num_heads\"],        # only used in Part II with the Transformer Encoder\n",
        "    config[\"dec_num_layers\"],\n",
        "    config[\"dec_num_heads\"],\n",
        "    config[\"d_model\"],\n",
        "    config[\"d_ff\"],\n",
        "    config[\"optimizer\"],\n",
        "    config[\"scheduler\"])\n",
        "\n",
        "if USE_WANDB:\n",
        "\n",
        "    wandb.login(key=\"3c7b273814544590b64c54d9a5242bde38616e02\", relogin=True) # TODO enter your key here\n",
        "\n",
        "    if RESUME_LOGGING:\n",
        "        run_id = \"\"\n",
        "        run = wandb.init(\n",
        "            id     = run_id,        ### Insert specific run id here if you want to resume a previous run\n",
        "            resume = True,          ### You need this to resume previous runs, but comment out reinit=True when using this\n",
        "            project = \"HW4P2-S24\",  ### Project should be created in your wandb account\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        run = wandb.init(\n",
        "            name    = run_name,     ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "            reinit  = True,         ### Allows reinitalizing runs when you re-run this cell\n",
        "            project = \"HW4P2-S24\",  ### Project should be created in your wandb account\n",
        "            config  = config        ### Wandb Config for your run\n",
        "        )\n",
        "\n",
        "        ### Save your model architecture as a string with str(model)\n",
        "        model_arch  = str(model)\n",
        "\n",
        "        ### Save it in a txt file\n",
        "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
        "        file_write  = arch_file.write(model_arch)\n",
        "        arch_file.close()\n",
        "\n",
        "        ### Log it in your wandb run with wandb.save()\n",
        "        # wandb.save(\"model_arch.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vUMtM_dVyrG"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1LD0uyPEHTI",
        "outputId": "7e5a9247-363b-4f41-be54-4eb838cb0870"
      },
      "outputs": [],
      "source": [
        "e                   = 0\n",
        "best_loss           = 20\n",
        "\n",
        "checkpoint_root = os.path.join(os.getcwd(), \"checkpoints-basic-transformer\")\n",
        "os.makedirs(checkpoint_root, exist_ok=True)\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "checkpoint_best_loss_model_filename     = 'checkpoint-best-loss-model.pth'\n",
        "checkpoint_last_epoch_filename          = 'checkpoint-epoch-'\n",
        "best_loss_model_path                    = os.path.join(checkpoint_root, checkpoint_best_loss_model_filename)\n",
        "\n",
        "if RESUME_LOGGING:\n",
        "    # change if you want to load best test model accordingly\n",
        "    checkpoint = torch.load(wandb.restore(checkpoint_best_loss_model_filename, run_path=\"\"+run_id).name)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    e = checkpoint['epoch']\n",
        "\n",
        "    print(\"Resuming from epoch {}\".format(e+1))\n",
        "    print(\"Epochs left: \", config['epochs']-e)\n",
        "    print(\"Optimizer: \\n\", optimizer)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "epochs = config[\"epochs\"]\n",
        "for epoch in range(e, epochs):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config[\"epochs\"]))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    train_loss, train_perplexity, attention_weights = train_model(model, train_loader, optimizer)\n",
        "\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Train Perplexity {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1, config[\"epochs\"], train_loss, train_perplexity, curr_lr))\n",
        "\n",
        "    if (epoch % 2 == 0):    # validate every 2 epochs to speed up training\n",
        "        levenshtein_distance = validate_fast(model, val_loader)\n",
        "        print(\"Levenshtein Distance {:.04f}\".format(levenshtein_distance))\n",
        "\n",
        "        wandb.log({\"train_loss\"     : train_loss,\n",
        "                \"train_perplexity\"  : train_perplexity,\n",
        "                \"learning_rate\"     : curr_lr,\n",
        "                \"val_distance\"      : levenshtein_distance})\n",
        "\n",
        "    else:\n",
        "        wandb.log({\"train_loss\"     : train_loss,\n",
        "                \"train_perplexity\"  : train_perplexity,\n",
        "                \"learning_rate\"     : curr_lr})\n",
        "\n",
        "    # plotting the encoder-nearest and decoder-nearest attention weights\n",
        "    attention_keys = list(attention_weights.keys())\n",
        "\n",
        "    attention_weights_decoder_self       = attention_weights[attention_keys[0]][0].cpu().detach().numpy()\n",
        "    attention_weights_decoder_cross      = attention_weights[attention_keys[-1]][0].cpu().detach().numpy()\n",
        "\n",
        "    # saving the cross-attention weights\n",
        "    save_attention_plot(attention_weights_decoder_cross, epoch+100)\n",
        "\n",
        "    # plot_attention_weights((attention_weights[attention_keys[0]][0]).cpu().detach().numpy())\n",
        "    # plot_attention_weights(attention_weights[attention_keys[-1]][0].cpu().detach().numpy())\n",
        "\n",
        "    if config[\"scheduler\"] == \"ReduceLR\":\n",
        "        scheduler.step(levenshtein_distance)\n",
        "    else:\n",
        "        scheduler.step()\n",
        "\n",
        "    # ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
        "    # epoch_model_path = os.path.join(checkpoint_root, (checkpoint_last_epoch_filename + str(epoch) + '.pth'))\n",
        "    # save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, epoch_model_path)\n",
        "    ## wandb.save(epoch_model_path) ## Can't save on wandb for all epochs, may blow up storage\n",
        "\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if best_loss >= train_loss:\n",
        "        best_loss = train_loss\n",
        "        save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, best_loss_model_path)\n",
        "        # wandb.save(best_loss_model_path)\n",
        "        print(\"Saved best training model\")\n",
        "\n",
        "### Finish your wandb run\n",
        "# run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GP8gtF7V2P-",
        "outputId": "4722bac1-147f-4235-99b4-15c9e28dbac4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2W3Dh78Adyg",
        "outputId": "6103ea6e-c3f0-48de-97a4-1e16ab401eb3"
      },
      "outputs": [],
      "source": [
        "valid_actual_distance = validate_full(model, val_loader)\n",
        "print(valid_actual_distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgFYFaBGeBqM"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVKku0SQW8y6"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# progress bar\n",
        "batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, leave=False, position=0, desc=\"Test\", ncols=5)\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "    inputs, inputs_lengths = data\n",
        "    inputs                 = inputs.to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        greedy_predictions = model.recognize(inputs, inputs_lengths)\n",
        "\n",
        "    greedy_predictions = greedy_predictions.detach().cpu().numpy()\n",
        "\n",
        "    for batch_idx in range(greedy_predictions.shape[0]):\n",
        "        pred_sliced = indices_to_chars(greedy_predictions[batch_idx], vocab= VOCAB)\n",
        "        pred_string = ''.join(pred_sliced)\n",
        "        all_predictions.append(pred_string)\n",
        "\n",
        "    batch_bar.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH9kvWIz5_9Z"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "df = pd.DataFrame({\n",
        "    \"index\" : list(range(len(all_predictions))),\n",
        "    \"label\" : all_predictions\n",
        "})\n",
        "\n",
        "df.to_csv(\"test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQP8QgXnAxqP"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c hw4p2-sp24 -f test.csv -m \"HW4P2 Preliminary Submission\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gyb5IUOu6Ba3"
      },
      "outputs": [],
      "source": [
        "# TODO Sumbit predictions to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eccQFlez6Kr3"
      },
      "source": [
        "# PART #2 Full Transformer (Encoder-Decoder)\n",
        "\n",
        "Using an LSTM encoder helps to capture some dependencies in the input sequence which helps the transformer decoder to decode. However, we can do more than that. The transformer encoder plays a crucial role by processing the input sequence into a high-dimensional space, capturing complex relationships between elements. It uses self-attention to understand the context around each feature, enabling the model to grasp subtle nuances and dependencies, crucial for our speech recognition task and improving performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIDOlW7v8BUk"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dooU-7ld8GZ4"
      },
      "source": [
        "Tips in completing this section\n",
        "\n",
        "* We will use the LSTM Encoder as input embedding.\n",
        "* How many multi-head attention modules do we have.\n",
        "* Can you seperate the encoder layer from the whole encoder architecture ?\n",
        "* How do  we implement skip  connections\n",
        "* How many layer norms do we have ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH7Lz9tPgtb0"
      },
      "source": [
        "## Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAw0eHIt0gez"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # @TODO: fill in the blanks appropriately (given the modules above)\n",
        "        self.mha        = MultiHeadAttention(num_heads, d_model, dropout=dropout)\n",
        "        self.ffn        = FeedForward(d_model, d_ff)\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1   = nn.Dropout(p=dropout)\n",
        "        self.dropout2   = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, inp, pad_mask, slf_attn_mask):\n",
        "\n",
        "        # Multi-Head Attention\n",
        "        #   (1) perform Multi-Head Attention on inp\n",
        "        output1, attn_weights1 = self.mha(inp, inp, inp, slf_attn_mask )\n",
        "        ''' TODO '''\n",
        "\n",
        "        # Skip (Residual) Connection\n",
        "        #   (1) perform dropout\n",
        "        output1 = self.dropout1(output1)\n",
        "        #   (2) add the input as a skip connection\n",
        "        ''' TODO '''\n",
        "\n",
        "        # Layer Normalization\n",
        "        #   (1) call layernorm on this resulting value\n",
        "        output1 = self.layernorm1(output1 +  inp)\n",
        "        ''' TODO '''\n",
        "\n",
        "        # Feed Forward Network\n",
        "        #   (1) apply feed forward layer\n",
        "        ffn_output = self.ffn(output1)\n",
        "        ''' TODO '''\n",
        "\n",
        "        # Skip (Residual) Connection\n",
        "        #   (1) perform dropout\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        #   (2) add the output of the first layernorm as a skip connection\n",
        "        output2 = self.layernorm2(ffn_output + output1)\n",
        "        ''' TODO '''\n",
        "\n",
        "        # Layer Normalization\n",
        "        #   (1) call layernorm on this resulting value\n",
        "        ''' TODO '''\n",
        "        return output2, attn_weights1\n",
        "        raise NotImplemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K0aEZhz0y8v"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_layers, d_model, num_heads, d_ff, mfcc_max_seq_length, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # @TODO: fill in the blanks appropriately (given the modules above and P1 setup)\n",
        "        self.embedding      = CNN_LSTM_Encoder(input_dim, 256, dropout)\n",
        "        self.projection     = torch.nn.Linear(1024, d_model)\n",
        "        self.pos_encoding   =  PositionalEncoding(d_model, max_len=mfcc_max_seq_length)\n",
        "\n",
        "        # concatenating all num_layers EncoderLayer blocks\n",
        "        self.enc_layers     = torch.nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.dropout        = nn.Dropout(dropout)\n",
        "        self.layernorm      = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, input_lengths):\n",
        "\n",
        "        # apply the CNN-LSTM Encoder (embedding)\n",
        "        ''' TODO '''\n",
        "        # print(\"x shape\", x.shape)\n",
        "        encoder_output, encoder_lens = self.embedding(x, input_lengths)\n",
        "        inp_embedded = self.projection(encoder_output)\n",
        "        # adding projection layer to change dimension of encoder_outputs to match d_model\n",
        "        ''' TODO '''\n",
        "\n",
        "        # apply Positional Encoding on these extracted features\n",
        "        ''' TODO '''\n",
        "        inp_embedded = self.pos_encoding(inp_embedded)\n",
        "        # apply dropout as regularization technique\n",
        "        ''' TODO '''\n",
        "        inp_embedded = self.dropout(inp_embedded)\n",
        "        pad_mask = create_mask_1(inp_embedded,  encoder_lens)\n",
        "        ''' TODO '''\n",
        "\n",
        "        # create an attention mask -- sets padded area attention weights to infinity\n",
        "\n",
        "\n",
        "        dec_enc_attn_mask = create_mask_3(encoder_output, encoder_lens,encoder_output.shape[1])\n",
        "\n",
        "        # passing inputs through Transformer Encoder blocks\n",
        "        ''' TODO '''\n",
        "        for i, enc_layer in enumerate(self.enc_layers):\n",
        "            inp_embedded, attn_weights1 = enc_layer(inp_embedded, pad_mask, dec_enc_attn_mask )\n",
        "\n",
        "        return inp_embedded, encoder_lens\n",
        "        raise NotImplemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPL9Xgl50AVr"
      },
      "outputs": [],
      "source": [
        "class FullTransformer(torch.nn.Module):\n",
        "    def __init__(self, input_dim, enc_num_layers, dec_num_layers, enc_num_heads, dec_num_heads,\n",
        "                 d_model, d_ff, target_vocab_size, eos_token, sos_token,\n",
        "                 pad_token, dropout=0.1, trans_max_seq_length=550, mfcc_max_seq_length=3260):\n",
        "\n",
        "        super(FullTransformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(input_dim, enc_num_layers, d_model, enc_num_heads, d_ff,\n",
        "                               mfcc_max_seq_length,  dropout)\n",
        "\n",
        "        self.decoder = Decoder(dec_num_layers, d_model, dec_num_heads, d_ff,\n",
        "                               dropout, target_vocab_size, trans_max_seq_length, eos_token, sos_token, pad_token)\n",
        "\n",
        "    def forward(self, padded_input, input_lengths, padded_target, target_lengths):\n",
        "        # passing through Encoder\n",
        "        encoder_output, encoder_lens = self.encoder(padded_input, input_lengths)\n",
        "\n",
        "        # passing Encoder output and Attention masks through Decoder\n",
        "        output, attention_weights = self.decoder(padded_target, encoder_output, encoder_lens)\n",
        "\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def recognize(self, inp, inp_len):\n",
        "        \"\"\" sequence-to-sequence greedy search -- decoding one utterance at a time \"\"\"\n",
        "\n",
        "        encoder_outputs, encoder_lens = self.encoder(inp, inp_len)\n",
        "        out = self.decoder.recognize_greedy_search(encoder_outputs, encoder_lens)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lam5bYQ8gxke"
      },
      "source": [
        "## Full Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qKfN3bj9qk7"
      },
      "outputs": [],
      "source": [
        "full_model = FullTransformer(\n",
        "input_dim                   = config[\"input_dim\"],\n",
        "enc_num_layers              = config[\"enc_num_layers\"],\n",
        "dec_num_layers              = config[\"dec_num_layers\"],\n",
        "enc_num_heads               = config[\"enc_num_heads\"],\n",
        "dec_num_heads               = config[\"dec_num_heads\"],\n",
        "\n",
        "d_model                     = config[\"d_model\"],\n",
        "d_ff                        = config[\"d_ff\"],\n",
        "\n",
        "target_vocab_size           = len(VOCAB),\n",
        "eos_token                   = EOS_TOKEN,\n",
        "sos_token                   = SOS_TOKEN,\n",
        "pad_token                   = PAD_TOKEN,\n",
        "\n",
        "dropout                     = config[\"enc_dropout\"],\n",
        "\n",
        "# decrease to a small number if you are just trying to implement the network\n",
        "# we've computed the max length of the transcripts for you\n",
        "trans_max_seq_length        = 550,\n",
        "mfcc_max_seq_length         = dataset_max_len).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETooy0d59__J"
      },
      "outputs": [],
      "source": [
        "### Load your best P1 model checkpoint\n",
        "# model.load_state_dict(torch.load('path to best P1 model checkpoint')['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUg3DDMA_Bap"
      },
      "source": [
        "Freezing model weights except for transformer encoder layers. This is like an initialization strategy so that the encoder learns the range of values required by the decoder. After about 3 epochs when this initialzation is done, unfreeze weights of the entire model and let it train.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjAH-p6B-Y2M"
      },
      "outputs": [],
      "source": [
        "### Copy weights from P1 model to full model\n",
        "### Freeze the weights of full transformer input embedding, linear projection, and decoder\n",
        "\n",
        "full_model.encoder.embedding.load_state_dict(model.encoder.state_dict())\n",
        "for param in full_model.encoder.embedding.parameters():\n",
        "    param.requires_grad = False # TODO make it non-trainable\n",
        "\n",
        "full_model.encoder.projection.load_state_dict(model.proj.state_dict())\n",
        "for param in full_model.encoder.projection.parameters():\n",
        "    param.requires_grad = False # TODO make it non-trainable\n",
        "\n",
        "full_model.decoder.load_state_dict(model.decoder.state_dict())\n",
        "for param in full_model.decoder.parameters():\n",
        "    param.requires_grad = False # TODO make it non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MHQLAwGar9x"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDLdbCwdghEB"
      },
      "source": [
        "## Loss, Optimizer, and Scheduler Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymlPz5lfghl3"
      },
      "outputs": [],
      "source": [
        "loss_func   = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN)\n",
        "scaler      = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGR9Rk2JgkQD"
      },
      "outputs": [],
      "source": [
        "''' defining optimizer '''\n",
        "\n",
        "if config[\"optimizer\"] == \"SGD\":\n",
        "  # feel free to change any of the initializations you like to fit your needs\n",
        "  optimizer = torch.optim.SGD(full_model.parameters(),\n",
        "                              lr=config[\"learning_rate\"],\n",
        "                              momentum=config[\"momentum\"],\n",
        "                              weight_decay=1E-4,\n",
        "                              nesterov=config[\"nesterov\"])\n",
        "\n",
        "elif config[\"optimizer\"] == \"Adam\":\n",
        "  # feel free to change any of the initializations you like to fit your needs\n",
        "  optimizer = torch.optim.Adam(full_model.parameters(),\n",
        "                               lr=float(config[\"learning_rate\"]),\n",
        "                               weight_decay=1e-4)\n",
        "\n",
        "elif config[\"optimizer\"] == \"AdamW\":\n",
        "  # feel free to change any of the initializations you like to fit your needs\n",
        "  optimizer = torch.optim.AdamW(full_model.parameters(), lr=float(config[\"learning_rate\"]),\n",
        "                                betas=(0.9, 0.999),\n",
        "                                eps=1e-9,\n",
        "                                weight_decay=0.01)\n",
        "\n",
        "''' defining scheduler '''\n",
        "\n",
        "if config[\"scheduler\"] == \"ReduceLR\":\n",
        "  #Feel Free to change any of the initializations you like to fit your needs\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                factor=config[\"factor\"], patience=config[\"patience\"], min_lr=1E-8, verbose=True)\n",
        "\n",
        "elif config[\"scheduler\"] == \"CosineAnnealing\":\n",
        "  #Feel Free to change any of the initializations you like to fit your needs\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                T_max = config[\"epochs\"], eta_min=1E-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoaXWl9IqUfi"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHEgNSYU_0tH"
      },
      "outputs": [],
      "source": [
        "###  Now let's train the encoder to master the encoder input ranges\n",
        "e                   = 0\n",
        "best_loss           = 10.0\n",
        "\n",
        "checkpoint_root = os.path.join(os.getcwd(), 'checkpointsfull')\n",
        "os.makedirs(checkpoint_root, exist_ok=True)\n",
        "wandb.watch(full_model, log=\"all\")\n",
        "\n",
        "checkpoint_best_loss_model_filename     = 'checkpoint-best-loss-modelfull.pth'\n",
        "checkpoint_last_epoch_filename          = 'checkpoint-epochfull-'\n",
        "best_loss_model_path                    = os.path.join(checkpoint_root, checkpoint_best_loss_model_filename)\n",
        "\n",
        "if RESUME_LOGGING:\n",
        "    # change if you want to load best test model accordingly\n",
        "    checkpoint = torch.load(wandb.restore(checkpoint_best_loss_model_filename, run_path=\"\"+run_id).name)\n",
        "\n",
        "    full_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    e = checkpoint['epoch']\n",
        "\n",
        "    print(\"Resuming from epoch {}\".format(e+1))\n",
        "    print(\"Epochs left: \", config['epochs']-e)\n",
        "    print(\"Optimizer: \\n\", optimizer)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "epochs = config[\"epochs\"]\n",
        "for epoch in range(e, epochs):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config[\"epochs\"]))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    train_loss, train_perplexity, attention_weights = train_model(full_model, train_loader, optimizer)\n",
        "\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Train Perplexity {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1, config[\"epochs\"], train_loss, train_perplexity, curr_lr))\n",
        "\n",
        "    if (epoch % 2 == 0):    # validate every 2 epochs to speed up training\n",
        "        levenshtein_distance = validate_fast(full_model, val_loader)\n",
        "        print(\"Levenshtein Distance {:.04f}\".format(levenshtein_distance))\n",
        "\n",
        "        wandb.log({\"train_loss\"     : train_loss,\n",
        "                \"train_perplexity\"  : train_perplexity,\n",
        "                \"learning_rate\"     : curr_lr,\n",
        "                \"val_distance\"      : levenshtein_distance})\n",
        "\n",
        "    else:\n",
        "        wandb.log({\"train_loss\"     : train_loss,\n",
        "                \"train_perplexity\"  : train_perplexity,\n",
        "                \"learning_rate\"     : curr_lr})\n",
        "\n",
        "    # plotting the encoder-nearest and decoder-nearest attention weights\n",
        "    attention_keys = list(attention_weights.keys())\n",
        "\n",
        "    # plot_attention_weights((attention_weights[attention_keys[0]][0]).cpu().detach().numpy())\n",
        "    # plot_attention_weights(attention_weights[attention_keys[-1]][0].cpu().detach().numpy())\n",
        "\n",
        "    attention_weights_decoder_self       = attention_weights[attention_keys[0]][0].cpu().detach().numpy()\n",
        "    attention_weights_decoder_cross      = attention_weights[attention_keys[-1]][0].cpu().detach().numpy()\n",
        "    save_attention_plot(attention_weights_decoder_cross, epoch)\n",
        "\n",
        "    if config[\"scheduler\"] == \"ReduceLR\":\n",
        "        scheduler.step(levenshtein_distance)\n",
        "    else:\n",
        "        scheduler.step()\n",
        "\n",
        "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
        "    epoch_model_path = os.path.join(checkpoint_root, (checkpoint_last_epoch_filename + str(epoch) + '.pth'))\n",
        "    save_model(full_model, optimizer, scheduler, ['train_loss', train_loss], epoch, epoch_model_path)\n",
        "    ## wandb.save(epoch_model_path) ## Can't save on wandb for all epochs, may blow up storage\n",
        "\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    # the first time we add the transformer encoder block, we freeze all other weights\n",
        "    #   except the transformer encoder weights and train this partially-frozen model\n",
        "    #   for about 3 epochs to weight initialize the transformer encoder weights.\n",
        "    #   This speeds up convergence of the entire network.\n",
        "    # if epoch == 3:##   WWATCH OUT\n",
        "    #     break\n",
        "\n",
        "    if best_loss <= train_loss:\n",
        "        best_loss = train_loss\n",
        "        save_model(full_model, optimizer, scheduler, ['train_loss', train_loss], epoch, best_loss_model_path)\n",
        "        # wandb.save(best_loss_model_path)\n",
        "        print(\"Saved best training model\")\n",
        "\n",
        "### Finish your wandb run\n",
        "# run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV72WOmGARVm"
      },
      "outputs": [],
      "source": [
        "validate_full(full_model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3qqn4nbHCdj"
      },
      "outputs": [],
      "source": [
        "### TODO Unfreeze the weights of full transformer input embedding and decoder\n",
        "# @NOTE: after unfreezing, continue training the P2 model for more epochs to achieve best results!\n",
        "\n",
        "for param in full_model.encoder.embedding.parameters():\n",
        "    param.requires_grad = True # TODO make it trainable\n",
        "\n",
        "for param in full_model.encoder.projection.parameters():\n",
        "    param.requires_grad = True # TODO make it trainable\n",
        "\n",
        "for param in full_model.decoder.parameters():\n",
        "    param.requires_grad = True # TODO make it trainable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2h4hspIreu7"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf-xgN8rBAWT"
      },
      "outputs": [],
      "source": [
        "full_model.eval()\n",
        "\n",
        "# progress bar\n",
        "batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, leave=False, position=0, desc=\"Test\", ncols=5)\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "    inputs, inputs_lengths = data\n",
        "    inputs                 = inputs.to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        greedy_predictions = full_model.recognize(inputs, inputs_lengths)\n",
        "\n",
        "    greedy_predictions = greedy_predictions.detach().cpu().numpy()\n",
        "\n",
        "    for batch_idx in range(greedy_predictions.shape[0]):\n",
        "        pred_sliced = indices_to_chars(greedy_predictions[batch_idx], vocab= VOCAB)\n",
        "        pred_string = ''.join(pred_sliced)\n",
        "        all_predictions.append(pred_string)\n",
        "\n",
        "    batch_bar.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwz_XRqnBTbr"
      },
      "outputs": [],
      "source": [
        "# %cd /content/\n",
        "df = pd.DataFrame({\n",
        "    \"index\" : list(range(len(all_predictions))),\n",
        "    \"label\" : all_predictions\n",
        "})\n",
        "\n",
        "df.to_csv(\"improved.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJD6b3AgBlT-"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c hw4p2-sp24 -f improved.csv -m \"It's been a ride :)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vacdiL_WCvgW"
      },
      "source": [
        "# Part #3 (Improving your Model)\n",
        "* Think of the input embedding, can you have something better than just an CNN-LSTM? Hint: HW3P2 might perform better.\n",
        "* Think about the transformer encoder parameter, what about number of heads and number of layers, can this help?\n",
        "* What about the decoder parameters what can  we improve there?\n",
        "* We trained using an incremental training technique, can we use this technique to add more layers?\n",
        "\n",
        "**Good Luck!!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hgFYFaBGeBqM",
        "eccQFlez6Kr3",
        "kH7Lz9tPgtb0",
        "Lam5bYQ8gxke",
        "PDLdbCwdghEB",
        "aoaXWl9IqUfi",
        "p2h4hspIreu7"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0533f79de9f049b7aa640fb6b53923d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06715b7f0ffe4c25925bd68b6f626de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d0cddff7d2949e0b2c8f533a4315db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10e4ef60303b4cbb948ab52ee84be605": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "159f2db1cce042c7807c0c8a86252666": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "173e63f02bac4d8f9c9a7085cabf6f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2edac223355466b9bd1ab170bcf3982",
              "IPY_MODEL_bad661cbe92f42e3b47d91fc63ef99cf",
              "IPY_MODEL_efaa2670b38f441fb703e05c6d2dd71d"
            ],
            "layout": "IPY_MODEL_8044c937b23048e8b7a6e9a727f9c21c"
          }
        },
        "178e5fc1784c443c8ab8bfe5481410c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c48abe4289941d68d8dd49f024a4e1a",
              "IPY_MODEL_6c599ea368724b02aa26ea2248d094b3",
              "IPY_MODEL_f24e25523d6d4b3b861304b51913884c"
            ],
            "layout": "IPY_MODEL_cb5a107a82944b3b9dc8ebb3743b735e"
          }
        },
        "3039b16bbbd5434eb85a2014e836c633": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d66a23d9534509a2f119a92b2b114a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3521f963ce2948c9a63270125659ea75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e13025db21c5478183fc45a12d1ab947",
              "IPY_MODEL_b4ad3a6341b84dd09e0bd185ed673c01",
              "IPY_MODEL_92cde89a1b7b4b99ac17d27b33eb5587"
            ],
            "layout": "IPY_MODEL_33d66a23d9534509a2f119a92b2b114a"
          }
        },
        "373c9e6af2634f72aa15ca32786748c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bdf1473bab1424896edb165a009af59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eb26c1a65b14a88a7399ac5c091e91a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4231389f56a84823a0ec9239911a5bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4324d33c846f456aa43b3de5725d46a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0a6316ab924e0797b9cc82bda742fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a413ef10196e4c3182ef963b2ad2d51a",
            "placeholder": "",
            "style": "IPY_MODEL_b05d8e951da94c98bd5a2e2b18c495f4",
            "value": "456k/456k[00:00&lt;00:00,32.7MB/s]"
          }
        },
        "693ea5a123b843b2b6bc3962274cfce0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c599ea368724b02aa26ea2248d094b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da71027381af42849bffb3d42a60c8c4",
            "max": 772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10e4ef60303b4cbb948ab52ee84be605",
            "value": 772
          }
        },
        "6dc977a0d3c54ec1a2e60844c49146a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4324d33c846f456aa43b3de5725d46a4",
            "placeholder": "",
            "style": "IPY_MODEL_93f1ef8ef6fd412583815998136f7ef2",
            "value": "899k/899k[00:00&lt;00:00,13.2MB/s]"
          }
        },
        "6ebd6b14277d4014becf95543018c89c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71459ce2ca81455c9997eb3b6eb9a92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e9bc34111ef436d8abdff79bbd76f92",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1c229a837fd4730918453065a823a32",
            "value": 456318
          }
        },
        "74c625b14182431bb71d1ad45eb31003": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b00fc0abae048bb88981e69728201b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c48abe4289941d68d8dd49f024a4e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bdf1473bab1424896edb165a009af59",
            "placeholder": "",
            "style": "IPY_MODEL_7ecb7c7f48794981aa2d9f6232fa232b",
            "value": "special_tokens_map.json:100%"
          }
        },
        "7ecb7c7f48794981aa2d9f6232fa232b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8044c937b23048e8b7a6e9a727f9c21c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "806d0f3f3ece45cd9fb82b9d62022358": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88715eac854d46ce805914b8c79a32e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373c9e6af2634f72aa15ca32786748c1",
            "placeholder": "",
            "style": "IPY_MODEL_06715b7f0ffe4c25925bd68b6f626de7",
            "value": "vocab.json:100%"
          }
        },
        "897f98d415394b3c9bc5eaed61ffcfb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eb26c1a65b14a88a7399ac5c091e91a",
            "placeholder": "",
            "style": "IPY_MODEL_7b00fc0abae048bb88981e69728201b2",
            "value": "merges.txt:100%"
          }
        },
        "8e9bc34111ef436d8abdff79bbd76f92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92cde89a1b7b4b99ac17d27b33eb5587": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7a3f99062d44556a752ebc542cde030",
            "placeholder": "",
            "style": "IPY_MODEL_159f2db1cce042c7807c0c8a86252666",
            "value": "224/224[00:00&lt;00:00,18.1kB/s]"
          }
        },
        "93f1ef8ef6fd412583815998136f7ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a23f989d62a464aba22000906a9063e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_897f98d415394b3c9bc5eaed61ffcfb1",
              "IPY_MODEL_71459ce2ca81455c9997eb3b6eb9a92e",
              "IPY_MODEL_5c0a6316ab924e0797b9cc82bda742fe"
            ],
            "layout": "IPY_MODEL_0533f79de9f049b7aa640fb6b53923d7"
          }
        },
        "a1c229a837fd4730918453065a823a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a413ef10196e4c3182ef963b2ad2d51a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b05d8e951da94c98bd5a2e2b18c495f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4ad3a6341b84dd09e0bd185ed673c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ebd6b14277d4014becf95543018c89c",
            "max": 224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74c625b14182431bb71d1ad45eb31003",
            "value": 224
          }
        },
        "bad661cbe92f42e3b47d91fc63ef99cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3039b16bbbd5434eb85a2014e836c633",
            "max": 1118,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe72220f07d049e992715ec9ffa79164",
            "value": 1118
          }
        },
        "bb17277267e0416383f08404eabf2954": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c371e8b3780b4f8294b113bb5e6b11f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb5a107a82944b3b9dc8ebb3743b735e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec9cf42f9564aff9e4eb37b0e69c227": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88715eac854d46ce805914b8c79a32e0",
              "IPY_MODEL_ff83fb1004124e4295989e9b92de42da",
              "IPY_MODEL_6dc977a0d3c54ec1a2e60844c49146a4"
            ],
            "layout": "IPY_MODEL_ed7f86f5de8b4407b26ebc7d7f631c02"
          }
        },
        "d43cc641d66849928302c3e3ad5c9a14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da71027381af42849bffb3d42a60c8c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e13025db21c5478183fc45a12d1ab947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c371e8b3780b4f8294b113bb5e6b11f9",
            "placeholder": "",
            "style": "IPY_MODEL_0d0cddff7d2949e0b2c8f533a4315db7",
            "value": "preprocessor_config.json:100%"
          }
        },
        "e6e05b1528ed404d94433e94b961d520": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7a3f99062d44556a752ebc542cde030": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb0406c519a9470caed2ca05c315b692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed7f86f5de8b4407b26ebc7d7f631c02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efaa2670b38f441fb703e05c6d2dd71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d43cc641d66849928302c3e3ad5c9a14",
            "placeholder": "",
            "style": "IPY_MODEL_4231389f56a84823a0ec9239911a5bdb",
            "value": "1.12k/1.12k[00:00&lt;00:00,111kB/s]"
          }
        },
        "f24e25523d6d4b3b861304b51913884c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e05b1528ed404d94433e94b961d520",
            "placeholder": "",
            "style": "IPY_MODEL_eb0406c519a9470caed2ca05c315b692",
            "value": "772/772[00:00&lt;00:00,75.0kB/s]"
          }
        },
        "f2edac223355466b9bd1ab170bcf3982": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_693ea5a123b843b2b6bc3962274cfce0",
            "placeholder": "",
            "style": "IPY_MODEL_bb17277267e0416383f08404eabf2954",
            "value": "tokenizer_config.json:100%"
          }
        },
        "fe72220f07d049e992715ec9ffa79164": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff83fb1004124e4295989e9b92de42da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_806d0f3f3ece45cd9fb82b9d62022358",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff9b40e257a34cb28691128f9f8e3fbe",
            "value": 898822
          }
        },
        "ff9b40e257a34cb28691128f9f8e3fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
