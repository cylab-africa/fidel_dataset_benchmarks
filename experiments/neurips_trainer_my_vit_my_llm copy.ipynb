{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset.dataloader import MyOcrDataloader, MyCustomOcrDataloader, OCRDataAugmentor\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "import wandb\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import gc\n",
    "from utils.utils import *\n",
    "from utils.charactertokenizer import CharacterTokenizer\n",
    "from jiwer import wer, cer\n",
    "from models.models import TrOCRMyDecoder\n",
    "from models.vit import ViT\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = ViT(image_size = (256, 1024), num_classes= None, mlp_dim=1024, patch_size=32, dim=512, depth= 4, heads = 4 )\n",
    "# img = torch.randn(1, 3, 256, 1024)\n",
    "\n",
    "# preds = encoder(img) # (1, 1000)\n",
    "# preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), '..')), \"config/main.yaml\")\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "task  = config[\"TRAIN_TASK\"]\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "TRAIN_PATH =config[task+\"_\"+\"TRAIN_PATH\"]\n",
    "VAL_PATH =config[task+\"_\"+\"VAL_PATH\"]\n",
    "IMG_ROOT = config[task+\"_\"+\"IMG_ROOT\"]\n",
    "\n",
    "MODEL_ID = config[\"MODEL_ID\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = resize_and_patch_image\n",
    "# model =VisionEncoderDecoderModel.from_pretrained(MODEL_ID).to(device)\n",
    "# model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
    "# model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# model.config.vocab_size = model.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"rasyosef/bert-amharic-tokenizer\")\n",
    "# tokenizer.tokenize(\"የዓለምአቀፉ ነጻ ንግድ መስፋፋት ድህነትን ለማሸነፍ በሚደረገው ትግል አንዱ ጠቃሚ መሣሪያ ሊሆን መቻሉ ብዙ የሚነገርለት ጉዳይ ነው።\")\n",
    "# tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check for data loader\n",
    "os.chdir(\"../dataset\")\n",
    "augmentor = OCRDataAugmentor()\n",
    "tokenizer = CharacterTokenizer.from_pretrained('/home/ubuntu/HandWritten_Amharic_English_OCR/Amharic_Char_Tokenizer2')\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/home/ubuntu/data/synthetic_data/amharic_tokenizer_hf\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"rasyosef/bert-amharic-tokenizer\")\n",
    "\n",
    "if tokenizer.bos_token_id== None:\n",
    "    print(\"setting tokenizer\")\n",
    "    special_tokens_dict = {\n",
    "    \"bos_token\": \"<sos>\",\n",
    "    \"eos_token\": \"<eos>\"\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "train_data = MyCustomOcrDataloader(TRAIN_PATH, preprocessor=processor, tokenizer  = tokenizer, img_root=IMG_ROOT, transform=augmentor)\n",
    "IMG_ROOT = config[task+\"_IMG_ROOT\"+\"_TEST\"]\n",
    "val_data = MyCustomOcrDataloader(VAL_PATH, preprocessor=processor, tokenizer  = tokenizer, img_root=IMG_ROOT)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    batch_size  = config['BATCH_SIZE'],\n",
    "    shuffle     = True,\n",
    "    collate_fn= train_data.collate_fn\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader    = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    batch_size  = config[\"BATCH_SIZE\"],\n",
    "    shuffle     = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_data.collate_fn\n",
    ")\n",
    "\n",
    "val_loader      = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    batch_size  = config[\"BATCH_SIZE\"],\n",
    "    shuffle     = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_data.collate_fn,\n",
    ")\n",
    "\n",
    "print(\"No. of Train Images   : \", train_data.__len__())\n",
    "print(\"Batch Size           : \", config[\"BATCH_SIZE\"])\n",
    "print(\"Train Batches        : \", train_loader.__len__())\n",
    "print(\"Val Batches          : \", val_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Sanity Check '''\n",
    "\n",
    "print(\"Checking the Shapes of the Data --\\n\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    x_pad, y_shifted_pad, y_golden_pad, x_len, y_len, = batch\n",
    "\n",
    "    print(f\"x_pad shape:\\t\\t{x_pad.shape}\")\n",
    "    print(f\"x_len shape:\\t\\t{x_len.shape}\\n\")\n",
    "\n",
    "    print(f\"y_shifted_pad shape:\\t{y_shifted_pad.shape}\")\n",
    "    print(f\"y_golden_pad shape:\\t{y_golden_pad.shape}\")\n",
    "    print(f\"y_len shape:\\t\\t{y_len.shape}\\n\")\n",
    "    plt.imshow(x_pad[0][0].permute((1,2,0)))\n",
    "    # print(y_shifted_pad)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Please refer to the config file and top sections to fill in the following '''\n",
    "\n",
    "model = TrOCRMyDecoder(\n",
    "input_dim                   = None,\n",
    "dec_num_layers              = config[\"dec_num_layers\"],\n",
    "dec_num_heads               = config[\"dec_num_heads\"],\n",
    "\n",
    "d_model                     = config[\"d_model\"],\n",
    "d_ff                        = config[\"d_ff\"],\n",
    "\n",
    "target_vocab_size           = len(tokenizer),\n",
    "eos_token                   = tokenizer.eos_token_id,\n",
    "sos_token                   = tokenizer.bos_token_id,\n",
    "pad_token                   = tokenizer.pad_token_id,\n",
    "\n",
    "enc_dropout                 = config[\"enc_dropout\"],\n",
    "dec_dropout                 = config[\"enc_dropout\"],\n",
    "\n",
    "# decrease to a small number if you are just trying to implement the network\n",
    "max_seq_length              = 512 , # Max sequence length for transcripts. Check data verification.\n",
    ").to(device)\n",
    "\n",
    "def num_parameters(mode):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params / 1E6\n",
    "\n",
    "para = num_parameters(model)\n",
    "print(\"#\"*10)\n",
    "print(f\"Model Parameters:\\n {para}\")\n",
    "print(\"#\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc=\"Train\")\n",
    "\n",
    "    total_loss          = 0\n",
    "    running_loss        = 0.0\n",
    "    running_perplexity  = 0.0\n",
    "\n",
    "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs          = inputs.to(device)\n",
    "        targets_shifted = targets_shifted.to(device)\n",
    "        targets_golden  = targets_golden.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # passing the minibatch through the model\n",
    "            # raw_predictions, attention_weights = model(inputs, inputs_lengths, targets_shifted, targets_lengths)\n",
    "            raw_predictions, attention_weights = model(inputs, inputs_lengths, targets_shifted, targets_lengths)\n",
    "\n",
    "\n",
    "            padding_mask = torch.logical_not(torch.eq(targets_shifted, tokenizer.pad_token_id))\n",
    "\n",
    "            # cast the mask to float32\n",
    "            padding_mask = padding_mask.float()\n",
    "            loss = loss_func(raw_predictions.transpose(1,2), targets_golden)*padding_mask\n",
    "            loss = loss.sum() / padding_mask.sum()\n",
    "\n",
    "        scaler.scale(loss).backward()   # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer)          # This is a replacement for optimizer.step()\n",
    "        scaler.update()                 # This is something added just for FP16\n",
    "\n",
    "        running_loss        += float(loss.item())\n",
    "        perplexity          = torch.exp(loss)\n",
    "        running_perplexity  += perplexity.item()\n",
    "\n",
    "        # online training monitoring\n",
    "        batch_bar.set_postfix(\n",
    "            loss = \"{:.04f}\".format(float(running_loss / (i + 1))),\n",
    "            perplexity = \"{:.04f}\".format(float(running_perplexity / (i + 1)))\n",
    "        )\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    running_loss        = float(running_loss / len(train_loader))\n",
    "    running_perplexity  = float(running_perplexity / len(train_loader))\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    return running_loss, running_perplexity, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fast(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    # progress bar\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc=\"Val\", ncols=5)\n",
    "\n",
    "    running_distance = 0.0\n",
    "    running_cer = 0.0\n",
    "    running_wer = 0.0\n",
    "    running_char_f1 = 0.0\n",
    "    running_word_f1 = 0.0\n",
    "\n",
    "\n",
    "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(dataloader):\n",
    "\n",
    "        inputs  = inputs.to(device)\n",
    "        targets_golden = targets_golden.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            greedy_predictions = model.recognize(inputs, inputs_lengths)\n",
    "\n",
    "        # calculating Levenshtein Distance\n",
    "        # @NOTE: modify the print_example to print more or less validation examples\n",
    "        dist, cer, wer, charf1, word_f1 = calc_edit_distance(greedy_predictions, targets_golden, targets_lengths, tokenizer, print_example=True)\n",
    "        running_distance += dist\n",
    "        running_cer += cer\n",
    "        running_wer += wer\n",
    "        running_char_f1 += charf1\n",
    "        running_word_f1 += word_f1\n",
    "\n",
    "\n",
    "        # online validation distance monitoring\n",
    "        batch_bar.set_postfix(\n",
    "            running_distance = \"{:.04f}\".format(float(running_distance / (i + 1))),\n",
    "            running_cer = \"{:.04f}\".format(float(running_cer / (i + 1))),\n",
    "            running_wer = \"{:.04f}\".format(float(running_wer / (i + 1))),\n",
    "            running_char_f1 = \"{:.04f}\".format(float(running_char_f1 / (i + 1))),\n",
    "            running_word_f1 = \"{:.04f}\".format(float(running_word_f1 / (i + 1)))\n",
    "\n",
    "        )\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i==4: break      # validating only upon first five batches\n",
    "\n",
    "    batch_bar.close()\n",
    "    running_distance /= 5\n",
    "    running_cer /= 5\n",
    "    running_wer /= 5\n",
    "    running_char_f1 /= 5\n",
    "    running_word_f1 /= 5\n",
    "\n",
    "    return running_distance, running_cer, running_wer, running_char_f1, running_word_f1\n",
    "\n",
    "def validate_full(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    # progress bar\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc=\"Val\", ncols=5)\n",
    "\n",
    "    running_distance = 0.0\n",
    "    running_cer = 0.0\n",
    "    running_wer = 0.0\n",
    "    running_char_f1 = 0.0\n",
    "    running_word_f1 =0.0\n",
    "\n",
    "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(dataloader):\n",
    "\n",
    "        inputs  = inputs.to(device)\n",
    "        targets_golden = targets_golden.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            greedy_predictions = model.recognize(inputs, inputs_lengths)\n",
    "\n",
    "        # calculating Levenshtein Distance\n",
    "        # @NOTE: modify the print_example to print more or less validation examples\n",
    "        dist, cer, wer, charf1, word_f1 = calc_edit_distance(greedy_predictions, targets_golden, targets_lengths, tokenizer, print_example=False)\n",
    "        running_distance += dist\n",
    "        running_cer += cer\n",
    "        running_wer += wer\n",
    "        running_char_f1 += charf1\n",
    "        running_word_f1 += word_f1\n",
    "\n",
    "\n",
    "        # online validation distance monitoring\n",
    "        batch_bar.set_postfix(\n",
    "            running_distance = \"{:.04f}\".format(float(running_distance / (i + 1))),\n",
    "            running_cer = \"{:.04f}\".format(float(running_cer / (i + 1))),\n",
    "            running_wer = \"{:.04f}\".format(float(running_wer / (i + 1))),\n",
    "            running_char_f1 = \"{:.04f}\".format(float(running_char_f1 / (i + 1))),\n",
    "            running_word_f1 = \"{:.04f}\".format(float(running_word_f1 / (i + 1)))\n",
    "\n",
    "        )\n",
    "        batch_bar.update()\n",
    "\n",
    "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    batch_bar.close()\n",
    "    running_distance /= len(dataloader)\n",
    "    running_cer /= len(dataloader)\n",
    "    running_wer /= len(dataloader)\n",
    "    running_char_f1 /= len(dataloader)\n",
    "    running_word_f1 /= len(dataloader)\n",
    "\n",
    "\n",
    "    return running_distance, running_cer, running_wer, running_char_f1, running_word_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' defining optimizer '''\n",
    "# vocab_size = len(tokenizer)\n",
    "# weights = torch.ones(vocab_size).to(\"cuda\")  # default weight = 1 for all\n",
    "# whitespace_token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' ')[0])\n",
    "# # Decrease weight for whitespace token\n",
    "# weights[whitespace_token_id] = 0.1  # e.g., reduce impact by 90%\n",
    "loss_func   = torch.nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "scaler      = torch.cuda.amp.GradScaler()\n",
    "if config[\"optimizer\"] == \"SGD\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.SGD(model.parameters(),\n",
    "                              lr=config[\"learning_rate\"],\n",
    "                              momentum=config[\"momentum\"],\n",
    "                              weight_decay=1E-4,\n",
    "                              nesterov=config[\"nesterov\"])\n",
    "\n",
    "elif config[\"optimizer\"] == \"Adam\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.Adam(model.parameters(),\n",
    "                               lr=float(config[\"learning_rate\"]),\n",
    "                               weight_decay=1e-4)\n",
    "\n",
    "elif config[\"optimizer\"] == \"AdamW\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                lr=float(config[\"learning_rate\"]),\n",
    "                                weight_decay=0.01)\n",
    "\n",
    "''' defining scheduler '''\n",
    "\n",
    "if config[\"scheduler\"] == \"ReduceLR\":\n",
    "  #Feel Free to change any of the initializations you like to fit your needs\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                factor=config[\"factor\"], patience=config[\"patience\"], min_lr=1E-8, verbose=True)\n",
    "\n",
    "elif config[\"scheduler\"] == \"CosineAnnealing\":\n",
    "  #Feel Free to change any of the initializations you like to fit your needs\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                T_max = 35, eta_min=1E-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using WandB? resume training?\n",
    "\n",
    "USE_WANDB = config[\"USE_WANDB\"]\n",
    "RESUME_LOGGING = False\n",
    "\n",
    "# creating your WandB run\n",
    "run_name = \"{}_Transformer_ENC-{}/{}_DEC-{}/{}_{}_{}_{}_{}\".format(\n",
    "    config[\"Name\"],\n",
    "    config[\"enc_num_layers\"],       # only used in Part II with the Transformer Encoder\n",
    "    config[\"enc_num_heads\"],        # only used in Part II with the Transformer Encoder\n",
    "    config[\"dec_num_layers\"],\n",
    "    config[\"dec_num_heads\"],\n",
    "    config[\"d_model\"],\n",
    "    config[\"d_ff\"],\n",
    "    config[\"optimizer\"],\n",
    "    config[\"scheduler\"])\n",
    "\n",
    "\n",
    "if USE_WANDB:\n",
    "\n",
    "    wandb.login(key=\"3c7b273814544590b64c54d9a5242bde38616e02\", relogin=True) # TODO enter your key here\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = \"\"\n",
    "        run = wandb.init(\n",
    "            id     = run_id,        ### Insert specific run id here if you want to resume a previous run\n",
    "            resume = True,          ### You need this to resume previous runs, but comment out reinit=True when using this\n",
    "            project = task+\"ocr-cnn-lstm\",  ### Project should be created in your wandb account\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name    = run_name,     ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            reinit  = True,         ### Allows reinitalizing runs when you re-run this cell\n",
    "            project = task+\"ocr-cnn-lstm\",  ### Project should be created in your wandb account\n",
    "            config  = config        ### Wandb Config for your run\n",
    "        )\n",
    "\n",
    "        ### Save your model architecture as a string with str(model)\n",
    "        model_arch  = str(model)\n",
    "\n",
    "        ### Save it in a txt file\n",
    "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "        file_write  = arch_file.write(model_arch)\n",
    "        arch_file.close()\n",
    "\n",
    "        ### Log it in your wandb run with wandb.save()\n",
    "        # wandb.save(\"model_arch.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*20)\n",
    "print(\"Task==\", task)\n",
    "\n",
    "# %%\n",
    "e                   = 0\n",
    "best_loss           = 0\n",
    "best_distance  = 0\n",
    "checkpoint_root = os.path.join(os.getcwd(), \"checkpoints-basic-cnn-trocr\")\n",
    "os.makedirs(checkpoint_root, exist_ok=True)\n",
    "if USE_WANDB:\n",
    "    wandb.watch(model, log=\"all\")\n",
    "checkpoint_best_loss_model_filename     = task +'checkpoint-best-loss-model.pth' \n",
    "checkpoint_best_distance_model_filename     = task +'checkpoint-best-distance-model.pth'\n",
    "\n",
    "checkpoint_last_epoch_filename          = 'checkpoint-epoch-'\n",
    "best_loss_model_path                    = os.path.join(checkpoint_root, checkpoint_best_loss_model_filename)\n",
    "best_distance_model_path                    = os.path.join(checkpoint_root, checkpoint_best_distance_model_filename)\n",
    "\n",
    "\n",
    "if RESUME_LOGGING:\n",
    "    # change if you want to load best test model accordingly\n",
    "    checkpoint = torch.load(wandb.restore(checkpoint_best_loss_model_filename, run_path=\"\"+run_id).name)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    e = checkpoint['epoch']\n",
    "\n",
    "    print(\"Resuming from epoch {}\".format(e+1))\n",
    "    print(\"Epochs left: \", config['epochs']-e)\n",
    "    print(\"Optimizer: \\n\", optimizer)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "epochs = config[\"epochs\"]\n",
    "for epoch in range(e, epochs):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config[\"epochs\"]))\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    train_loss, train_perplexity, attention_weights = train_model(model, train_loader, optimizer)\n",
    "\n",
    "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Train Perplexity {:.04f}\\t Learning Rate {:.04f}\".format(\n",
    "        epoch + 1, config[\"epochs\"], train_loss, train_perplexity, curr_lr))\n",
    "\n",
    "    if (epoch >1 )and (epoch % 5 == 0):    # validate every 2 epochs to speed up training\n",
    "        levenshtein_distance, cers, wers, charf1, wordf1 = validate_fast(model, val_loader)\n",
    "        if best_distance <= cers:\n",
    "            best_distance = cers\n",
    "            save_model(model, optimizer, scheduler, ['val_distance', cers], epoch, best_distance_model_path)\n",
    "            # wandb.save(best_loss_model_path)\n",
    "            print(\"Saved distance training model\")\n",
    "        print(\"cer {:.04f}\".format(cers))\n",
    "        if USE_WANDB:\n",
    "            wandb.log({\"train_loss\"     : train_loss,\n",
    "                    \"train_perplexity\"  : train_perplexity,\n",
    "                    \"learning_rate\"     : curr_lr,\n",
    "                    \"val_distance\"      : cer,\n",
    "                    \"charf1\": charf1,\n",
    "                    \"wordf1\": wordf1,})\n",
    "\n",
    "    else:\n",
    "        if USE_WANDB:\n",
    "\n",
    "            wandb.log({\"train_loss\"     : train_loss,\n",
    "                    \"train_perplexity\"  : train_perplexity,\n",
    "                    \"learning_rate\"     : curr_lr})\n",
    "\n",
    "    # if config[\"scheduler\"] == \"ReduceLR\":\n",
    "    #     scheduler.step(levenshtein_distance)\n",
    "    # else:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    if best_loss >= train_loss:\n",
    "        best_loss = train_loss\n",
    "        save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, best_loss_model_path)\n",
    "        # wandb.save(best_loss_model_path)\n",
    "        print(\"Saved best training model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(best_distance_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "result = {}\n",
    "for task in [\"TYPED\", \"HANDWRITTEN\", \"SYNTHETIC\",  \"HDD\"]:\n",
    "    print(f\"{config[\"TRAIN_TASK\"]} is been evaluated on {task}\")\n",
    "    VAL_PATH =config[task+\"_VAL_PATH\"]\n",
    "    IMG_ROOT = config[task+\"_IMG_ROOT\"+\"_TEST\"]\n",
    "\n",
    "    val_data = MyCustomOcrDataloader(VAL_PATH, preprocessor=processor, tokenizer  = tokenizer, img_root=IMG_ROOT)\n",
    "    val_loader      = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    batch_size  = config[\"BATCH_SIZE\"],\n",
    "    shuffle     = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_data.collate_fn,\n",
    "    )\n",
    "    levenshtein_distance, cers, wers, charf1, wordf1 = validate_full(model, val_loader)\n",
    "    result[task]  = {}\n",
    "    for metric, score in zip ([\"lev\", \"cers\", \"wers\", \"charf1\", \"wordf1\"], [levenshtein_distance, cers, wers, charf1, wordf1]):\n",
    "        result[task][metric] = score\n",
    "\n",
    "    with open(f\"result{config[\"TRAIN_TASK\"]}_.json\", 'w') as file:\n",
    "        json.dump(result, file, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
