{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset.dataloader import MyOcrDataloader, MyCustomOcrDataloader, OCRDataAugmentor\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "import wandb\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import gc\n",
    "from utils.utils import *\n",
    "from utils.charactertokenizer import CharacterTokenizer\n",
    "from jiwer import wer, cer\n",
    "from models.models import TrOCRMyDecoder\n",
    "from models.vit import ViT\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = ViT(image_size = (256, 1024), num_classes= None, mlp_dim=1024, patch_size=32, dim=512, depth= 4, heads = 4 )\n",
    "# img = torch.randn(1, 3, 256, 1024)\n",
    "\n",
    "# preds = encoder(img) # (1, 1000)\n",
    "# preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), '..')), \"config/main.yaml\")\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "task  = config[\"TRAIN_TASK\"]\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "TRAIN_PATH =config[task+\"_\"+\"TRAIN_PATH\"]\n",
    "VAL_PATH =config[task+\"_\"+\"VAL_PATH\"]\n",
    "IMG_ROOT = config[task+\"_\"+\"IMG_ROOT\"]\n",
    "\n",
    "MODEL_ID = config[\"MODEL_ID\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = resize_and_patch_image\n",
    "# model =VisionEncoderDecoderModel.from_pretrained(MODEL_ID).to(device)\n",
    "# model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
    "# model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# model.config.vocab_size = model.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"rasyosef/bert-amharic-tokenizer\")\n",
    "# tokenizer.tokenize(\"የዓለምአቀፉ ነጻ ንግድ መስፋፋት ድህነትን ለማሸነፍ በሚደረገው ትግል አንዱ ጠቃሚ መሣሪያ ሊሆን መቻሉ ብዙ የሚነገርለት ጉዳይ ነው።\")\n",
    "# tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check for data loader\n",
    "os.chdir(\"../dataset\")\n",
    "augmentor = OCRDataAugmentor()\n",
    "tokenizer = CharacterTokenizer.from_pretrained('/home/ubuntu/HandWritten_Amharic_English_OCR/Amharic_Char_Tokenizer2')\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/home/ubuntu/data/synthetic_data/amharic_tokenizer_hf\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"rasyosef/bert-amharic-tokenizer\")\n",
    "\n",
    "if tokenizer.bos_token_id== None:\n",
    "    print(\"setting tokenizer\")\n",
    "    special_tokens_dict = {\n",
    "    \"bos_token\": \"<sos>\",\n",
    "    \"eos_token\": \"<eos>\"\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "train_data = MyCustomOcrDataloader(TRAIN_PATH, preprocessor=processor, tokenizer  = tokenizer, img_root=IMG_ROOT, transform=augmentor)\n",
    "IMG_ROOT = config[task+\"_IMG_ROOT\"+\"_TEST\"]\n",
    "val_data = MyCustomOcrDataloader(VAL_PATH, preprocessor=processor, tokenizer  = tokenizer, img_root=IMG_ROOT)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    batch_size  = config['BATCH_SIZE'],\n",
    "    shuffle     = True,\n",
    "    collate_fn= train_data.collate_fn\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader    = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    batch_size  = config[\"BATCH_SIZE\"],\n",
    "    shuffle     = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_data.collate_fn\n",
    ")\n",
    "\n",
    "val_loader      = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    batch_size  = config[\"BATCH_SIZE\"],\n",
    "    shuffle     = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_data.collate_fn,\n",
    ")\n",
    "\n",
    "print(\"No. of Train Images   : \", train_data.__len__())\n",
    "print(\"Batch Size           : \", config[\"BATCH_SIZE\"])\n",
    "print(\"Train Batches        : \", train_loader.__len__())\n",
    "print(\"Val Batches          : \", val_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Sanity Check '''\n",
    "\n",
    "print(\"Checking the Shapes of the Data --\\n\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    x_pad, y_shifted_pad, y_golden_pad, x_len, y_len, = batch\n",
    "\n",
    "    print(f\"x_pad shape:\\t\\t{x_pad.shape}\")\n",
    "    print(f\"x_len shape:\\t\\t{x_len.shape}\\n\")\n",
    "\n",
    "    print(f\"y_shifted_pad shape:\\t{y_shifted_pad.shape}\")\n",
    "    print(f\"y_golden_pad shape:\\t{y_golden_pad.shape}\")\n",
    "    print(f\"y_len shape:\\t\\t{y_len.shape}\\n\")\n",
    "    plt.imshow(x_pad[0][0].permute((1,2,0)))\n",
    "    # print(y_shifted_pad)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Please refer to the config file and top sections to fill in the following '''\n",
    "\n",
    "model = TrOCRMyDecoder(\n",
    "input_dim                   = None,\n",
    "dec_num_layers              = config[\"dec_num_layers\"],\n",
    "dec_num_heads               = config[\"dec_num_heads\"],\n",
    "\n",
    "d_model                     = config[\"d_model\"],\n",
    "d_ff                        = config[\"d_ff\"],\n",
    "\n",
    "target_vocab_size           = len(tokenizer),\n",
    "eos_token                   = tokenizer.eos_token_id,\n",
    "sos_token                   = tokenizer.bos_token_id,\n",
    "pad_token                   = tokenizer.pad_token_id,\n",
    "\n",
    "enc_dropout                 = config[\"enc_dropout\"],\n",
    "dec_dropout                 = config[\"enc_dropout\"],\n",
    "pre_train=True,\n",
    "# decrease to a small number if you are just trying to implement the network\n",
    "max_seq_length              = 512 , # Max sequence length for transcripts. Check data verification.\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# checkpoint = torch.load(\"/home/ubuntu/HandWritten_Amharic_English_OCR/dataset/checkpoints-llm-basic-cnn-transformer/TYPEDcheckpoint-best-loss-model.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "def num_parameters(mode):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params / 1E6\n",
    "\n",
    "para = num_parameters(model)\n",
    "print(\"#\"*10)\n",
    "print(f\"Model Parameters:\\n {para}\")\n",
    "print(\"#\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### freeze encoder\n",
    "# full_model.encoder.embedding.load_state_dict(model.encoder.state_dict())\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False # TODO make it non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc=\"Train\")\n",
    "\n",
    "    total_loss          = 0\n",
    "    running_loss        = 0.0\n",
    "    running_perplexity  = 0.0\n",
    "\n",
    "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs          = inputs.to(device)\n",
    "        targets_shifted = targets_shifted.to(device)\n",
    "        targets_golden  = targets_golden.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # passing the minibatch through the model\n",
    "            # raw_predictions, attention_weights = model(inputs, inputs_lengths, targets_shifted, targets_lengths)\n",
    "            raw_predictions, attention_weights = model(inputs, inputs_lengths, targets_shifted, targets_lengths)\n",
    "\n",
    "\n",
    "            padding_mask = torch.logical_not(torch.eq(targets_shifted, tokenizer.pad_token_id))\n",
    "\n",
    "            # cast the mask to float32\n",
    "            padding_mask = padding_mask.float()\n",
    "            loss = loss_func(raw_predictions.transpose(1,2), targets_golden)*padding_mask\n",
    "            loss = loss.sum() / padding_mask.sum()\n",
    "\n",
    "        scaler.scale(loss).backward()   # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer)          # This is a replacement for optimizer.step()\n",
    "        scaler.update()                 # This is something added just for FP16\n",
    "\n",
    "        running_loss        += float(loss.item())\n",
    "        perplexity          = torch.exp(loss)\n",
    "        running_perplexity  += perplexity.item()\n",
    "\n",
    "        # online training monitoring\n",
    "        batch_bar.set_postfix(\n",
    "            loss = \"{:.04f}\".format(float(running_loss / (i + 1))),\n",
    "            perplexity = \"{:.04f}\".format(float(running_perplexity / (i + 1)))\n",
    "        )\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    running_loss        = float(running_loss / len(train_loader))\n",
    "    running_perplexity  = float(running_perplexity / len(train_loader))\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    return running_loss, running_perplexity, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_fast(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    # progress bar\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc=\"Val\", ncols=5)\n",
    "\n",
    "    running_distance = 0.0\n",
    "    running_cer = 0.0\n",
    "    running_wer = 0.0\n",
    "    running_char_f1 = 0.0\n",
    "    running_word_f1 = 0.0\n",
    "\n",
    "\n",
    "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(dataloader):\n",
    "\n",
    "        inputs  = inputs.to(device)\n",
    "        targets_golden = targets_golden.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            greedy_predictions = model.recognize(inputs, inputs_lengths)\n",
    "\n",
    "        # calculating Levenshtein Distance\n",
    "        # @NOTE: modify the print_example to print more or less validation examples\n",
    "        dist, cer, wer, charf1, word_f1 = calc_edit_distance(greedy_predictions, targets_golden, targets_lengths, tokenizer, print_example=True)\n",
    "        running_distance += dist\n",
    "        running_cer += cer\n",
    "        running_wer += wer\n",
    "        running_char_f1 += charf1\n",
    "        running_word_f1 += word_f1\n",
    "\n",
    "\n",
    "        # online validation distance monitoring\n",
    "        batch_bar.set_postfix(\n",
    "            running_distance = \"{:.04f}\".format(float(running_distance / (i + 1))),\n",
    "            running_cer = \"{:.04f}\".format(float(running_cer / (i + 1))),\n",
    "            running_wer = \"{:.04f}\".format(float(running_wer / (i + 1))),\n",
    "            running_char_f1 = \"{:.04f}\".format(float(running_char_f1 / (i + 1))),\n",
    "            running_word_f1 = \"{:.04f}\".format(float(running_word_f1 / (i + 1)))\n",
    "\n",
    "        )\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i==4: break      # validating only upon first five batches\n",
    "\n",
    "    batch_bar.close()\n",
    "    running_distance /= 5\n",
    "    running_cer /= 5\n",
    "    running_wer /= 5\n",
    "    running_char_f1 /= 5\n",
    "    running_word_f1 /= 5\n",
    "\n",
    "    return running_distance, running_cer, running_wer, running_char_f1, running_word_f1\n",
    "\n",
    "def validate_full(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    # progress bar\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc=\"Val\", ncols=5)\n",
    "\n",
    "    running_distance = 0.0\n",
    "    running_cer = 0.0\n",
    "    running_wer = 0.0\n",
    "    running_char_f1 = 0.0\n",
    "    running_word_f1 =0.0\n",
    "\n",
    "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(dataloader):\n",
    "\n",
    "        inputs  = inputs.to(device)\n",
    "        targets_golden = targets_golden.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            greedy_predictions = model.recognize(inputs, inputs_lengths)\n",
    "\n",
    "        # calculating Levenshtein Distance\n",
    "        # @NOTE: modify the print_example to print more or less validation examples\n",
    "        dist, cer, wer, charf1, word_f1 = calc_edit_distance(greedy_predictions, targets_golden, targets_lengths, tokenizer, print_example=False)\n",
    "        running_distance += dist\n",
    "        running_cer += cer\n",
    "        running_wer += wer\n",
    "        running_char_f1 += charf1\n",
    "        running_word_f1 += word_f1\n",
    "\n",
    "\n",
    "        # online validation distance monitoring\n",
    "        batch_bar.set_postfix(\n",
    "            running_distance = \"{:.04f}\".format(float(running_distance / (i + 1))),\n",
    "            running_cer = \"{:.04f}\".format(float(running_cer / (i + 1))),\n",
    "            running_wer = \"{:.04f}\".format(float(running_wer / (i + 1))),\n",
    "            running_char_f1 = \"{:.04f}\".format(float(running_char_f1 / (i + 1))),\n",
    "            running_word_f1 = \"{:.04f}\".format(float(running_word_f1 / (i + 1)))\n",
    "\n",
    "        )\n",
    "        batch_bar.update()\n",
    "\n",
    "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    batch_bar.close()\n",
    "    running_distance /= len(dataloader)\n",
    "    running_cer /= len(dataloader)\n",
    "    running_wer /= len(dataloader)\n",
    "    running_char_f1 /= len(dataloader)\n",
    "    running_word_f1 /= len(dataloader)\n",
    "\n",
    "\n",
    "    return running_distance, running_cer, running_wer, running_char_f1, running_word_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' defining optimizer '''\n",
    "# vocab_size = len(tokenizer)\n",
    "# weights = torch.ones(vocab_size).to(\"cuda\")  # default weight = 1 for all\n",
    "# whitespace_token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' ')[0])\n",
    "# # Decrease weight for whitespace token\n",
    "# weights[whitespace_token_id] = 0.1  # e.g., reduce impact by 90%\n",
    "loss_func   = torch.nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "scaler      = torch.cuda.amp.GradScaler()\n",
    "if config[\"optimizer\"] == \"SGD\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.SGD(model.parameters(),\n",
    "                              lr=config[\"learning_rate\"],\n",
    "                              momentum=config[\"momentum\"],\n",
    "                              weight_decay=1E-4,\n",
    "                              nesterov=config[\"nesterov\"])\n",
    "\n",
    "elif config[\"optimizer\"] == \"Adam\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.Adam(model.parameters(),\n",
    "                               lr=float(config[\"learning_rate\"]),\n",
    "                               weight_decay=1e-4)\n",
    "\n",
    "elif config[\"optimizer\"] == \"AdamW\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                lr=float(config[\"learning_rate\"]),\n",
    "                                weight_decay=0.01)\n",
    "\n",
    "''' defining scheduler '''\n",
    "\n",
    "if config[\"scheduler\"] == \"ReduceLR\":\n",
    "  #Feel Free to change any of the initializations you like to fit your needs\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                factor=config[\"factor\"], patience=config[\"patience\"], min_lr=1E-8, verbose=True)\n",
    "\n",
    "elif config[\"scheduler\"] == \"CosineAnnealing\":\n",
    "  #Feel Free to change any of the initializations you like to fit your needs\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                T_max = 35, eta_min=1E-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using WandB? resume training?\n",
    "\n",
    "USE_WANDB = config[\"USE_WANDB\"]\n",
    "RESUME_LOGGING = False\n",
    "\n",
    "# creating your WandB run\n",
    "run_name = \"{}_Transformer_ENC-{}/{}_DEC-{}/{}_{}_{}_{}_{}\".format(\n",
    "    config[\"Name\"],\n",
    "    config[\"enc_num_layers\"],       # only used in Part II with the Transformer Encoder\n",
    "    config[\"enc_num_heads\"],        # only used in Part II with the Transformer Encoder\n",
    "    config[\"dec_num_layers\"],\n",
    "    config[\"dec_num_heads\"],\n",
    "    config[\"d_model\"],\n",
    "    config[\"d_ff\"],\n",
    "    config[\"optimizer\"],\n",
    "    config[\"scheduler\"])\n",
    "task = \"handonly\"\n",
    "\n",
    "if USE_WANDB:\n",
    "\n",
    "    wandb.login(key=\"3c7b273814544590b64c54d9a5242bde38616e02\", relogin=True) # TODO enter your key here\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = \"\"\n",
    "        run = wandb.init(\n",
    "            id     = run_id,        ### Insert specific run id here if you want to resume a previous run\n",
    "            resume = True,          ### You need this to resume previous runs, but comment out reinit=True when using this\n",
    "            project = task+\"ocr-cnn-lstm\",  ### Project should be created in your wandb account\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name    = run_name,     ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            reinit  = True,         ### Allows reinitalizing runs when you re-run this cell\n",
    "            project = task+\"ocr-cnn-lstm\",  ### Project should be created in your wandb account\n",
    "            config  = config        ### Wandb Config for your run\n",
    "        )\n",
    "\n",
    "        ### Save your model architecture as a string with str(model)\n",
    "        model_arch  = str(model)\n",
    "\n",
    "        ### Save it in a txt file\n",
    "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "        file_write  = arch_file.write(model_arch)\n",
    "        arch_file.close()\n",
    "\n",
    "        ### Log it in your wandb run with wandb.save()\n",
    "        # wandb.save(\"model_arch.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(epoch+\u001b[32m1\u001b[39m, epochs))\n\u001b[32m     39\u001b[39m curr_lr = \u001b[38;5;28mfloat\u001b[39m(optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m train_loss, train_perplexity, attention_weights = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrain Loss \u001b[39m\u001b[38;5;132;01m{:.04f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m Train Perplexity \u001b[39m\u001b[38;5;132;01m{:.04f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m Learning Rate \u001b[39m\u001b[38;5;132;01m{:.04f}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m     44\u001b[39m     epoch + \u001b[32m1\u001b[39m, config[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m], train_loss, train_perplexity, curr_lr))\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# if (epoch >-1 )and (epoch % 10 == 0):    # validate every 2 epochs to speed up training\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m#     levenshtein_distance, cer, wer, charf1, wordf1 = validate_fast(model, val_loader)\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m#     if best_distance <= levenshtein_distance:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, epoch_model_path)\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m## wandb.save(epoch_model_path) ## Can't save on wandb for all epochs, may blow up storage\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, optimizer)\u001b[39m\n\u001b[32m      7\u001b[39m running_loss        = \u001b[32m0.0\u001b[39m\n\u001b[32m      8\u001b[39m running_perplexity  = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_shifted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_golden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datagen/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datagen/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datagen/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1410\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1410\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1412\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datagen/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "e                   = 0\n",
    "best_loss           = 1000\n",
    "best_distance  = 0\n",
    "checkpoint_root = os.path.join(os.getcwd(), \"checkpoints-llm-basic-cnn-transformer\")\n",
    "os.makedirs(checkpoint_root, exist_ok=True)\n",
    "if USE_WANDB:\n",
    "    wandb.watch(model, log=\"all\")\n",
    "task =  config[\"TRAIN_TASK\"]\n",
    "checkpoint_best_loss_model_filename     = task +'checkpoint-best-loss-model.pth' \n",
    "checkpoint_best_distance_model_filename     = task +'checkpoint-best-distance-model.pth'\n",
    "\n",
    "checkpoint_last_epoch_filename          = 'checkpoint-epoch-'\n",
    "best_loss_model_path                    = os.path.join(checkpoint_root, checkpoint_best_loss_model_filename)\n",
    "best_distance_model_path                    = os.path.join(checkpoint_root, checkpoint_best_distance_model_filename)\n",
    "\n",
    "\n",
    "if RESUME_LOGGING:\n",
    "    # change if you want to load best test model accordingly\n",
    "    checkpoint = torch.load(wandb.restore(checkpoint_best_loss_model_filename, run_path=\"\"+run_id).name)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    e = checkpoint['epoch']\n",
    "\n",
    "    print(\"Resuming from epoch {}\".format(e+1))\n",
    "    print(\"Epochs left: \", config['epochs']-e)\n",
    "    print(\"Optimizer: \\n\", optimizer)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# epochs = config[\"epochs\"]\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(e, epochs):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    train_loss, train_perplexity, attention_weights = train_model(model, train_loader, optimizer)\n",
    "\n",
    "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Train Perplexity {:.04f}\\t Learning Rate {:.04f}\".format(\n",
    "        epoch + 1, config[\"epochs\"], train_loss, train_perplexity, curr_lr))\n",
    "\n",
    "    # if (epoch >-1 )and (epoch % 10 == 0):    # validate every 2 epochs to speed up training\n",
    "    #     levenshtein_distance, cer, wer, charf1, wordf1 = validate_fast(model, val_loader)\n",
    "    #     if best_distance <= levenshtein_distance:\n",
    "    #         best_distance = levenshtein_distance\n",
    "    #         save_model(model, optimizer, scheduler, ['val_distance', levenshtein_distance], epoch, best_distance_model_path)\n",
    "    #         # wandb.save(best_loss_model_path)\n",
    "    #         print(\"Saved distance training model\")\n",
    "    #     print(\"Levenshtein Distance {:.04f}\".format(levenshtein_distance))\n",
    "    #     if USE_WANDB:\n",
    "    #         wandb.log({\"train_loss\"     : train_loss,\n",
    "    #                 \"train_perplexity\"  : train_perplexity,\n",
    "    #                 \"learning_rate\"     : curr_lr,\n",
    "    #                 \"val_distance\"      : levenshtein_distance,\n",
    "    #                 \"charf1\": charf1,\n",
    "    #                 \"wordf1\": wordf1,})\n",
    "\n",
    "    # else:\n",
    "    #     if USE_WANDB:\n",
    "\n",
    "    #         wandb.log({\"train_loss\"     : train_loss,\n",
    "    #                 \"train_perplexity\"  : train_perplexity,\n",
    "    #                 \"learning_rate\"     : curr_lr})\n",
    "\n",
    "    # # plotting the encoder-nearest and decoder-nearest attention weights\n",
    "    # attention_keys = list(attention_weights.keys())\n",
    "\n",
    "    # attention_weights_decoder_self       = attention_weights[attention_keys[0]][0].cpu().detach().numpy()\n",
    "    # attention_weights_decoder_cross      = attention_weights[attention_keys[-1]][0].cpu().detach().numpy()\n",
    "\n",
    "    # # saving the cross-attention weights\n",
    "    # save_attention_plot(attention_weights_decoder_cross, epoch+100)\n",
    "\n",
    "    # plot_attention_weights((attention_weights[attention_keys[0]][0]).cpu().detach().numpy())\n",
    "    # plot_attention_weights(attention_weights[attention_keys[-1]][0].cpu().detach().numpy())\n",
    "\n",
    "    # if config[\"scheduler\"] == \"ReduceLR\":\n",
    "    #     scheduler.step(levenshtein_distance)\n",
    "    # else:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    # ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
    "    # epoch_model_path = os.path.join(checkpoint_root, (checkpoint_last_epoch_filename + str(epoch) + '.pth'))\n",
    "    # save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, epoch_model_path)\n",
    "    ## wandb.save(epoch_model_path) ## Can't save on wandb for all epochs, may blow up storage\n",
    "\n",
    "\n",
    "    if best_loss >= train_loss:\n",
    "        best_loss = train_loss\n",
    "        save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, best_loss_model_path)\n",
    "        # wandb.save(best_loss_model_path)\n",
    "        print(\"Saved best training model\")\n",
    "\n",
    "### Finish your wandb run\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sweeper eval\n",
    "# checkpoint = torch.load(best_distance_model_path)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "result = {}\n",
    "for task in [\"HANDWRITTEN\", \"SYNTHETIC\", \"TYPED\"]:\n",
    "    print(f\"{config[\"TRAIN_TASK\"]} is been evaluated on {task}\")\n",
    "    VAL_PATH =config[task+\"_VAL_PATH\"]\n",
    "    IMG_ROOT = config[task+\"_IMG_ROOT\"+\"_TEST\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    val_data = MyCustomOcrDataloader(VAL_PATH, preprocessor=processor, tokenizer  = tokenizer, img_root=IMG_ROOT)\n",
    "    val_loader      = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    batch_size  = config[\"BATCH_SIZE\"],\n",
    "    shuffle     = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_data.collate_fn,\n",
    "    )\n",
    "    levenshtein_distance, cer, wer, charf1, wordf1 = validate_full(model, val_loader)\n",
    "    result[task]  = {}\n",
    "    for metric, score in zip ([\"lev\", \"cer\", \"wer\", \"charf1\", \"wordf1\"], [levenshtein_distance, cer, wer, charf1, wordf1]):\n",
    "        result[task][metric] = score\n",
    "\n",
    "    with open(f\"result{config[\"TRAIN_TASK\"]}_.json\", 'w') as file:\n",
    "        json.dump(result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
