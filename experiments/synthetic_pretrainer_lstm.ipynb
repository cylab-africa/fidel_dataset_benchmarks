{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/datagen/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config {'SCENARIO': 1, 'DATASET_PATH': '/data/synthetic_data/labels/output_labels.csv', 'TRAIN_PATH': '/data/synthetic_data/labels/output_labels_train.csv', 'VAL_PATH': '/data/synthetic_data/labels/output_labels_test.csv', 'BATCH_SIZE': 64, 'MODEL_ID': 'microsoft/trocr-base-handwritten', 'EPOCHS': 100, 'batch_size': 256, 'enc_dropout': 0.2, 'enc_num_layers': 1, 'enc_num_heads': 1, 'dec_dropout': 0.2, 'dec_num_layers': 4, 'dec_num_heads': 4, 'd_model': 512, 'd_ff': 2048, 'learning_rate': '1E-4', 'optimizer': 'AdamW', 'momentum': 0.0, 'nesterov': True, 'scheduler': 'CosineAnnealing', 'factor': 0.9, 'patience': 6, 'epochs': 100, 'Name': 'blessed'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset.dataloader import MyOcrDataloader, MyCustomOcrDataloader\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "import wandb\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import gc\n",
    "from utils.utils import *\n",
    "from utils.charactertokenizer import CharacterTokenizer\n",
    "from jiwer import wer, cer\n",
    "from models.models import TrOCRMyDecoder\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), '..')), \"config/main.yaml\")\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "DATASET_PATH =config[\"DATASET_PATH\"]\n",
    "TRAIN_PATH =config[\"TRAIN_PATH\"]\n",
    "VAL_PATH =config[\"VAL_PATH\"]\n",
    "\n",
    "\n",
    "MODEL_ID = config[\"MODEL_ID\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = TrOCRProcessor.from_pretrained(MODEL_ID)\n",
    "# model =VisionEncoderDecoderModel.from_pretrained(MODEL_ID).to(device)\n",
    "# model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
    "# model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# model.config.vocab_size = model.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 3, 384, 384]) torch.Size([64, 119]) ['ከምሽቱ ሰዓት ጀምሮ በሩ ከውጭ ተቆልፎ እስከ ንጋት በመተኛት አለፈ በኛ በኩል ደግሞ ለወደፊቱ ሊኖር የምችለው ምርመራ ምን ሊሆን ይችላል', 'እኔ አንተ እና ቡድንህ ይህን በጣም ጥሩ የበረራ ኮምፒዩተር ክለሳ ስልኩን የሚባለውን መሳሪያ እንድትጠቀም እጋብዛችኋለሁ የሚገርም ነው ከፕሮጀክቱ በተወሰኑ ቅንጥቦች', 'ሸክማችንን አያቀለውም ትንንሽ ሐገር ከመሆን የስነ ልቦና ቀውስ ውስጥ ከመዘፈቅና በተገነጠልን ማግስት በድንበር ወይም በድንበር ተሻጋሪ ወንዞች ሰበብ ለእልቂት ከመዘጋጀት', 'በዚህም ምክንያት ከኬንያ ወንዞች ወደ ሐይቁ የሚገባው የውሃ መጠን ቀንሷል::', 'ለመሆኑ አሁን ያለህበት ሁኔታ ይህንን ነገር የሚያናግር ነው እስኪ አስብበት ታክሲ ውስጥ ነህአውቶቡስ ውስጥ ነህ ቢሮ ነህ ቤተ ክርስቲያን ነህ', 'ከዛም ቀጠሉ በዚህ ስብሰባ ላይ ያልፈለገአሞራ ብቻ ነው ብዙ እጆች ከተሰብሳቢው ተቀስረው ይታያሉጫጫታና ጉምጉምታውቀጠለ አንድ ጊዜ ተረጋጉ አሉ መለስ በአዳራሹ', 'ያዳህ እንግዳወርቅ ተፈራ ከቄስ ሳሙኤል በቀለ ጋር ክፍል::', 'ከዚህ በታች ይገኛሉ::', 'መጽሐፍ ቅዱስ ውስጥ በዳንኤል ላይ የሚገኘው ሐሳብ ግራ ስላጋባቸው አምላክ ይህን ጥቅስ የሚያብራራላቸው ሰው እንዲልክላቸው ጸለዩ::', 'በመሆኑም የኮንስትራክሽን ግብዓት አቅራቢዎችን ጨምሮ ሁሉንም ባለድርሻ አካላት የያዘ ፎረም ተቋቁሞ ችግሮችን በዘላቂነት ለመፍታት መስራት እንዳለበት ጠቁመዋል::', 'ካጠኑ በኋላ በደቡብ ጀርመንዋ ሽቱትጋርት በሚገኝ የኢንጅነሪንግ መሥሪያ ቤት ለሁለት ዓመት ሠርተዋል ጀርመን ሲኖሩ ከጀመሩ ተኛ አመታቸውን ያስቆጠሩት ኢንጅነር ገበያው', 'በአማኑኤል ሆስፒታል የሚፈጸመው በደል በከፊል በግልጽ የሚታይና የሚዳሰስ ነው ወያኔ በስውር የሚፈጽማቸው በደሎች ጣራ ይነካሉ ከተያዘው አጀንዳ መካከል ጥቂቱ የአማራ', 'የፀዳ የሥራ ከባቢ መፍጠር ሙያና ክህሎትን ለተጨባጭ ውጤት መጠቀም ሕግን ማስከበርና በቡድን መሥራትን ያመለክታሉ::', 'ጉንፋንንና ሌሎች መሰል የላይኛው የመተንፈሻ አካላት በሽታዎችን ይከላከላል ይፈውሳልም::', 'መጽሐፍ ቅዱስን ያነበቡ ሰዎች ሁሉ የሕይወት መለወጥ የኑሮ መቀየር የበሽታ ፈውስ የመንፈስ መረጋጋትን የልብ ስብራት መጠገንን የብቸኝነትን መወገድንና የተመሰቃቀለ ኑሮን', 'ሲባዝን ይታያል የወያኔ አምባገነናዊ መንግስት በዉሸት የተካነ አፋኝ ገዳይእውነት የሚያሰፈራው እንዲሁም የኢትዮጵያ ህዝብ አንድነት እና የታሪክ ጠላት መሆኑ የአደባባይ ሚስጥር', 'ይገጥማቸውና መንደራቸውን ለቀው ወደ ሌላ ቦታ ይሰደዳሉ እማ ጦጤእማ ጦጤ እናት ጦጣ ልጆቿን ሁሉ ይዛ ተሰደደች በመንገዳቸውም እየሄዱ ሳለ እማ', 'ወደ ቤቱም ወስዶ ማዕድ አቀረበላቸው በአምላክ በማመኑም ከመላው ቤተሰቡ ጋር እጅግ ተደሰተ::', 'ትዕዛዝ ነው ባለፉት ሁለት ቀናት ስንሰማው የነበረው ተመሳሳይ መልስ ተሰጠኝ::', 'አሻሮ ይዞ ወደ ቆሎ እንደተጠጋው ብልጥ አንድ ወር ያልሞላው ፓርቲ ይዘው እንደ መኢአድ ካሉ ፓርቲዎች ጋር ተቀናጅቼ እሰራለሁ አሉ በመሀል', 'ስለሆነ በቶሎ ተግባብተው ችግር ከሚፈቱበት ጊዜ ይልቅ በራሳቸው ችግርና ሥራ ተወጥረው ኑሮን ለማሸነፍ የሚጥሩበት በጊዜ የዕለት ፍጆታቸውን ለማሟላት የሚፋጠኑበት ጊዜ', 'ኢደመነፍሳዊ ዕውቀት አይወረስም በደም ወይም በዘር አይተላለፍም የዚህ ጥሩ ማስተላለፊያው መጽሐፍ ነው የአንድ ሳይንቲስት ልጅ ሲወለድ ያባቱን አንድም ዕውቀት ይዞ', 'መካሄዱን የአዲስ አበባ ፕሮጀክት ጽቤት የህዝብ ግንኙነት ሃላፊ አቶ ዳዊት ንጉሴ ለዶቼ ቬለ ተናግረዋል::', 'የዓለም የፕሬስ ነጻነት ባለፈው የአውሮፓውያን ዓመተ ምህረት በአስራ ሦስት ዓመታት ውስጥ ባልታየ ደረጃ አሽቆልቁሉዋል ሲል ጽህፈት ቤቱ ዩናይትድ ስቴትስ የሆነው', 'መጨማተሬ ይመሰክርብኛል ክሳቴም ተነሥቶብኛል በፊቴም ይመሰክርብኛል::', 'ከዚህ በተለየ በርካታ ችግሮች ስላሉብን ወደፊት ማኅበሩ አይዟችሁ እያለ ከጎናችን እንዳይለየን ብለዋል::', 'መልስ በእግዚአብሔር ሉዓላዊነትና በሰዎች ነጻ ፍቃድና ኃላፊነት መካከል ያለውን ግንኙነት ሙሉ ለመረዳት ያስቸግረናል እግዚአብሔር ብቻ ነው በእውነት የሚያውቀው እነሱ አንድ', 'መሬት ለሽያጭ ሳሪስ ቤሬጊግ የ ሜትር ካሬ አዲስ አበባ::', 'ኛ አብርሃ ደስታ ወጣቱን ፍርኃት የለሹን እና ልዩ ተሰጥኦ ያለውን ኢትዮጵያዊ ጦማሪ አብርሀም ደስታን ከልብ አከብራለሁ አሁን በቅርቡ እኤአ ጁላይ', 'አክሎም በተጠርጣሪዎች ላይ የምስክሮችን ቃል መቀበልና ሰነድ ማሰባሰብን እንዲሁም የኦዲት ሪፖርቶችንና የባለሙያ አስተያየቶችን እየሰበሰበ መሆኑን በመጥቀስ የምርመራ መዝገቡን በማደራጀት ስራ', 'ማን አለ እንለዋለን::', 'ድሉን ጨብጠን ትግሉ እስከ ነፃነት ይቀጥላል::', 'ወይም የአባይን ግድብ በጀት እጥፍ ማለት ነው::', 'ይህች የምርጫ ሂደት ምን አስታወሰችኝ መሰላችሁ ካልተሳሳትኩ ግዜው ይመስለኛልየመላው ኢትዮጵያ አንድነት ድርጅት መኢአድ በአይቤክስ ሆቴል ጠቅላላ ጉባዔውን እያካሄደ ነበር በዛውም', 'ሮደስ ታደሰ መጽሐፈ ሰዓታት ንባብና ትርጓሜው በአንድምታ ዓም::', 'በብዙ ሚሊዮን ዶላር የሚገመት ሐብት ንብረት ወድሟል::', 'እስካሁን ድረስ ኢትዮጵያን በሰው ዘር መገኛነት የሚቀድማት የለም::', 'ጉዳዮች ጽህፈት ቤት በዛሬው እለት ባወጣው::', 'አማራጭ አንድ ብቻ ነው ያም እኔው መናገር አለብኝ::', 'የኪነጥበቡን ማኅበረሰብ ሚና አመላካች ብቻ ሳይሆን የትውልዱን እውነተኛ ድምፅ ሆኗልትውልዱ የቴዲ አድማጭ ብቻ ሳይሆን ቴዲ የሚለውን ከፍ አድርጎ ማስተጋባት እና', 'እንዲሰጥ የሚያደርገው ሲናሚልዲሃይድ የተባለው ውህድ ይዘቱ ከፍተኛ መሆኑ ነው ከዚህ በተጨማሪም ቀረፋ የአንቲኦክሲደንትነት ባህሪው ጠንካራ ነው አንቲኦክሱደንቶች ሰውነትን ከተለያዩ ጂ', 'አጥናለሁ ሁልጊዜ የመረጥከኝም አንተው ነህ ለዚህ ስራ ፪::', 'የሳይንሰ መሣሪያዎችን ጥገና ብሔራዊ አቅም መገንባትና የጥገና አገልግሎት መስጠት::', 'አለኝ መሪዎቻችን ከዚህ ክስተት መማር ብቻ ሳይሆን የተማሩትን በተግባር ፈፅመው ከጎናቸው ያለውን ህዝባቸውን እንደሚያስደስቱቤተክህነቱም እንደዚሁ::', 'ዋርነር በጊዜው የሰሜንና ማዕከላዊ አሜሪካና ካሪቢያን እግር ኳስ ኮንፌዴሬሽን ፕሬዝዳንት ነበሩ ጃክ ዋርነር ከሌሎች የፊፋ ባለስልጣናት ጋር በቀረበባቸው የማጭበርበርህገወጥ የገንዘብ', 'ከዚህ ውጭ ኢዴአፓ ስለኢህአዴግ መጠናከር ብቻ ሳይሆን ስለአብዮታዊ ዴሞክራሲ ፍልስፍና ድክመት ሕገ መንግሥቱ ከዝግጅት ሂደት ጀምሮ በሀገራችን ህዝብ ውስጥ የአንድነት', 'በቅርብ ጊዜ ያልተነበበ መልእክት አሳይ በሮፒተር ፒስተን ማሽን አማካኝነት በቁጥጥር አወጣጥ::', 'ነው ጥያቄው ተገቢ ስላልሆነም ተገቢ ያልሆነ ስራ እየፈጸመ ነው::', 'መጫወት አልቻልንም ርዕዮትንም መጠየቅ ሰዓት ረፈደ እንደዚህ አድርገው አዋክበው ስልችቶን እንድንቀር ጠያቂ አጣው ብሎ አንዱዓለምን ተስፋ እንዲቆርጥ ነው እቅዱ ከአንዱዓለም', 'ለፍትሕ በእውነት የሚሠራ ከሆነ ዕርምጃ እንዲወስድ ጠይቋል ፍርድ ቤቱ ለአንዱዓለም አራጌ በሰጠው ምላሽ ብዙ ጊዜ ፍርድ ቤቱን እየደፈረ መሆኑን ጠቅሶ', 'ሕዝቡን ቀና ብለው አዩ ቀጥለው ንበስ አሉ ታሪካዊውን ንግግራቸውን ማሰማት ጀመሩ የሐገራቸው በእብሪተኞች መወረር ሥርዓተመንግሥቱ ተንዶ በባዕዳን ቁጥጥር ስር መግባቱ', 'ሽፋን የሃገር እና የሕዝብ በታኝ ሃይሎች ለራሳችን አሊያም ለሌላው ሻማ ሆነን በጋራ አብርተን በጋራ ቀልጠን ለመስዋትነት ያልተዘጋጀን የፖለቲካ ዲስኩራም ብኩኖች', 'ለደረቅ ፀጉር ልስላሴ እና ለፀጉር እድገት የሚሆን የፀጉር ማስክ::', 'እንዚያ ትጉህ ባይሆን ኖሮ ይገርመኝ ነበር::', 'አጭር መግለጫ መላክ ነው በእርግጥ የጀርባ ማገናኛ ጣቢያዎች እና ማውጫዎች ትክክለኛ ዝርዝር እንዳላችሁ ነውሁሉም ደብዳቤዎችዎ ከሚመች ርዕስ እና ተገቢ አገናኞች', 'መሆናቸውን መርማሪ ቡድኑ ወንጀል ተፈጽሟል የሚልበት ጊዜና እሳቸው የተቀጠሩበት ጊዜ ስለማይገናኝ የታሰሩት በሌሉበትና በማይገናኙበት መሆኑን ጠቁመው በነፃ እንዲሰናበቱ ፍርድ ቤቱን', 'የአሞን ልጆች በዳዊት ዘንድ እንደተጠሉ ባዩ ጊዜ የአሞን ልጆች ልከው ከሶርያውያን ከቤትሮዖብና ከሱባ ሀያ ሺህ እግረኞች ከመዓካ ንጉሥም አንድ ሺህ', 'ስደት እንደ ግለሰቦች ጉዳይ ተደርጎ ነበር የተወሰደው ግን አሁንም የገዳማት መደፈር የአብያተ ክርስቲያናት መቃጠል እንደ ቀጠለ ነው አሁን እኛ ክዚህ', 'ምድር ገነት እንደምትሆን የሚገልጸው ተስፋ ሕይወቴን ለወጠው::', 'የየራስ ኃላፊነቶችን በመወጣት ደረጃ ግንኙነት ሲደረግ መተጋገዝ ይሆናል::', 'ከፓወር ሼል የሁሉንም ድራይቭ ይዘት እንዴት ማረጋገጥ እችላለሁ::', 'ከዚህም በማያያዝ ወደ ቤትም ስትገቡ ሰላምታ ስጡ ቤቱም የሚገባው ቢሆን ሰላማችሁ ይድረስለት ባይገባው ግን ሰላማችሁ ይመለስላችሁ::', 'ጥናታዊ ዕውቅና አላገኘም ነበር እንጂ እንዲሁ በገደምዳሜው ግን ስሙ ይታወቅ ነበር ኢሕአዴግ እና ቲፎዞዎቹ ተቃዋሚዎች ልማቱን ይክዳሉ ይላሉ ልማቱ የኢሕአዴግ', 'ሞት ብቻ እንደሆነ::']\n"
     ]
    }
   ],
   "source": [
    "## sanity check for data loader\n",
    "os.chdir(\"../dataset\")\n",
    "tokenizer = CharacterTokenizer.from_pretrained('/home/ubuntu/HandWritten_Amharic_English_OCR/Amharic_Char_Tokenizer2')\n",
    "train_data = MyCustomOcrDataloader(TRAIN_PATH, preprocessor=processor, tokenizer  = tokenizer, img_root='/data/synthetic_data/data')\n",
    "val_data = MyCustomOcrDataloader(VAL_PATH, preprocessor=processor, tokenizer  = tokenizer, img_root='/data/synthetic_data/data')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    batch_size  = config['BATCH_SIZE'],\n",
    "    shuffle     = True,\n",
    "    collate_fn= train_data.collate_fn\n",
    "    )\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch[0].shape, batch[1].shape,tokenizer.batch_decode(batch[1], skip_special_tokens=True))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Train Images   :  268029\n",
      "Batch Size           :  256\n",
      "Train Batches        :  1047\n",
      "Val Batches          :  117\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader    = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    batch_size  = config[\"batch_size\"],\n",
    "    shuffle     = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_data.collate_fn\n",
    ")\n",
    "\n",
    "val_loader      = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    batch_size  = config[\"batch_size\"],\n",
    "    shuffle     = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_data.collate_fn,\n",
    ")\n",
    "\n",
    "print(\"No. of Train Images   : \", train_data.__len__())\n",
    "print(\"Batch Size           : \", config[\"batch_size\"])\n",
    "print(\"Train Batches        : \", train_loader.__len__())\n",
    "print(\"Val Batches          : \", val_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Shapes of the Data --\n",
      "\n",
      "x_pad shape:\t\ttorch.Size([256, 1, 3, 384, 384])\n",
      "x_len shape:\t\ttorch.Size([256])\n",
      "\n",
      "y_shifted_pad shape:\ttorch.Size([256, 124])\n",
      "y_golden_pad shape:\ttorch.Size([256, 124])\n",
      "y_len shape:\t\ttorch.Size([256])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Sanity Check '''\n",
    "\n",
    "print(\"Checking the Shapes of the Data --\\n\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    x_pad, y_shifted_pad, y_golden_pad, x_len, y_len, = batch\n",
    "\n",
    "    print(f\"x_pad shape:\\t\\t{x_pad.shape}\")\n",
    "    print(f\"x_len shape:\\t\\t{x_len.shape}\\n\")\n",
    "\n",
    "    print(f\"y_shifted_pad shape:\\t{y_shifted_pad.shape}\")\n",
    "    print(f\"y_golden_pad shape:\\t{y_golden_pad.shape}\")\n",
    "    print(f\"y_len shape:\\t\\t{y_len.shape}\\n\")\n",
    "\n",
    "    # print(y_shifted_pad)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/datagen/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/datagen/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "Model Parameters:\n",
      " 28.871575\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "''' Please refer to the config file and top sections to fill in the following '''\n",
    "\n",
    "model = TrOCRMyDecoder(\n",
    "input_dim                   = None,\n",
    "dec_num_layers              = config[\"dec_num_layers\"],\n",
    "dec_num_heads               = config[\"dec_num_heads\"],\n",
    "\n",
    "d_model                     = config[\"d_model\"],\n",
    "d_ff                        = config[\"d_ff\"],\n",
    "\n",
    "target_vocab_size           = tokenizer.vocab_size,\n",
    "eos_token                   = tokenizer.eos_token_id,\n",
    "sos_token                   = tokenizer.bos_token_id,\n",
    "pad_token                   = tokenizer.pad_token_id,\n",
    "\n",
    "enc_dropout                 = config[\"enc_dropout\"],\n",
    "dec_dropout                 = config[\"enc_dropout\"],\n",
    "\n",
    "# decrease to a small number if you are just trying to implement the network\n",
    "max_seq_length              = 200 , # Max sequence length for transcripts. Check data verification.\n",
    ").to(device)\n",
    "\n",
    "def num_parameters(mode):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params / 1E6\n",
    "\n",
    "para = num_parameters(model)\n",
    "print(\"#\"*10)\n",
    "print(f\"Model Parameters:\\n {para}\")\n",
    "print(\"#\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc=\"Train\")\n",
    "\n",
    "    total_loss          = 0\n",
    "    running_loss        = 0.0\n",
    "    running_perplexity  = 0.0\n",
    "\n",
    "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs          = inputs.to(device)\n",
    "        targets_shifted = targets_shifted.to(device)\n",
    "        targets_golden  = targets_golden.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # passing the minibatch through the model\n",
    "            # raw_predictions, attention_weights = model(inputs, inputs_lengths, targets_shifted, targets_lengths)\n",
    "            raw_predictions, attention_weights = model(inputs, inputs_lengths, targets_shifted, targets_lengths)\n",
    "\n",
    "\n",
    "            padding_mask = torch.logical_not(torch.eq(targets_shifted, tokenizer.pad_token_id))\n",
    "\n",
    "            # cast the mask to float32\n",
    "            padding_mask = padding_mask.float()\n",
    "            loss = loss_func(raw_predictions.transpose(1,2), targets_golden)*padding_mask\n",
    "            loss = loss.sum() / padding_mask.sum()\n",
    "\n",
    "        scaler.scale(loss).backward()   # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer)          # This is a replacement for optimizer.step()\n",
    "        scaler.update()                 # This is something added just for FP16\n",
    "\n",
    "        running_loss        += float(loss.item())\n",
    "        perplexity          = torch.exp(loss)\n",
    "        running_perplexity  += perplexity.item()\n",
    "\n",
    "        # online training monitoring\n",
    "        batch_bar.set_postfix(\n",
    "            loss = \"{:.04f}\".format(float(running_loss / (i + 1))),\n",
    "            perplexity = \"{:.04f}\".format(float(running_perplexity / (i + 1)))\n",
    "        )\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    running_loss        = float(running_loss / len(train_loader))\n",
    "    running_perplexity  = float(running_perplexity / len(train_loader))\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    return running_loss, running_perplexity, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fast(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    # progress bar\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc=\"Val\", ncols=5)\n",
    "\n",
    "    running_distance = 0.0\n",
    "\n",
    "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(dataloader):\n",
    "\n",
    "        inputs  = inputs.to(device)\n",
    "        targets_golden = targets_golden.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            greedy_predictions = model.recognize(inputs, inputs_lengths)\n",
    "\n",
    "        # calculating Levenshtein Distance\n",
    "        # @NOTE: modify the print_example to print more or less validation examples\n",
    "        running_distance += calc_edit_distance(greedy_predictions, targets_golden, targets_lengths, tokenizer, print_example=True)\n",
    "\n",
    "        # online validation distance monitoring\n",
    "        batch_bar.set_postfix(\n",
    "            running_distance = \"{:.04f}\".format(float(running_distance / (i + 1)))\n",
    "        )\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if i==4: break      # validating only upon first five batches\n",
    "\n",
    "    batch_bar.close()\n",
    "    running_distance /= 5\n",
    "\n",
    "    return running_distance\n",
    "\n",
    "def validate_full(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    # progress bar\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc=\"Val\", ncols=5)\n",
    "\n",
    "    running_distance = 0.0\n",
    "\n",
    "    for i, (inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths) in enumerate(dataloader):\n",
    "\n",
    "        inputs  = inputs.to(device)\n",
    "        targets_golden = targets_golden.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            greedy_predictions = model.recognize(inputs, inputs_lengths)\n",
    "\n",
    "        # calculating Levenshtein Distance\n",
    "        # @NOTE: modify the print_example to print more or less validation examples\n",
    "        running_distance += calc_edit_distance(greedy_predictions, targets_golden, targets_lengths, tokenizer, print_example=True)\n",
    "\n",
    "        # online validation distance monitoring\n",
    "        batch_bar.set_postfix(\n",
    "            running_distance = \"{:.04f}\".format(float(running_distance / (i + 1)))\n",
    "        )\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "        del inputs, targets_shifted, targets_golden, inputs_lengths, targets_lengths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    batch_bar.close()\n",
    "    running_distance /= len(dataloader)\n",
    "\n",
    "    return running_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193076/3515402767.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler      = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "''' defining optimizer '''\n",
    "loss_func   = torch.nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "scaler      = torch.cuda.amp.GradScaler()\n",
    "if config[\"optimizer\"] == \"SGD\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.SGD(model.parameters(),\n",
    "                              lr=config[\"learning_rate\"],\n",
    "                              momentum=config[\"momentum\"],\n",
    "                              weight_decay=1E-4,\n",
    "                              nesterov=config[\"nesterov\"])\n",
    "\n",
    "elif config[\"optimizer\"] == \"Adam\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.Adam(model.parameters(),\n",
    "                               lr=float(config[\"learning_rate\"]),\n",
    "                               weight_decay=1e-4)\n",
    "\n",
    "elif config[\"optimizer\"] == \"AdamW\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                lr=float(config[\"learning_rate\"]),\n",
    "                                weight_decay=0.01)\n",
    "\n",
    "''' defining scheduler '''\n",
    "\n",
    "if config[\"scheduler\"] == \"ReduceLR\":\n",
    "  #Feel Free to change any of the initializations you like to fit your needs\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                factor=config[\"factor\"], patience=config[\"patience\"], min_lr=1E-8, verbose=True)\n",
    "\n",
    "elif config[\"scheduler\"] == \"CosineAnnealing\":\n",
    "  #Feel Free to change any of the initializations you like to fit your needs\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                T_max = 35, eta_min=1E-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using WandB? resume training?\n",
    "\n",
    "USE_WANDB = False\n",
    "RESUME_LOGGING = False\n",
    "\n",
    "# creating your WandB run\n",
    "run_name = \"{}_Transformer_ENC-{}/{}_DEC-{}/{}_{}_{}_{}_{}\".format(\n",
    "    config[\"Name\"],\n",
    "    config[\"enc_num_layers\"],       # only used in Part II with the Transformer Encoder\n",
    "    config[\"enc_num_heads\"],        # only used in Part II with the Transformer Encoder\n",
    "    config[\"dec_num_layers\"],\n",
    "    config[\"dec_num_heads\"],\n",
    "    config[\"d_model\"],\n",
    "    config[\"d_ff\"],\n",
    "    config[\"optimizer\"],\n",
    "    config[\"scheduler\"])\n",
    "\n",
    "if USE_WANDB:\n",
    "\n",
    "    wandb.login(key=\"3c7b273814544590b64c54d9a5242bde38616e02\", relogin=True) # TODO enter your key here\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = \"\"\n",
    "        run = wandb.init(\n",
    "            id     = run_id,        ### Insert specific run id here if you want to resume a previous run\n",
    "            resume = True,          ### You need this to resume previous runs, but comment out reinit=True when using this\n",
    "            project = \"ocr-cnn-lstm\",  ### Project should be created in your wandb account\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name    = run_name,     ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            reinit  = True,         ### Allows reinitalizing runs when you re-run this cell\n",
    "            project = \"ocr-cnn-lstm\",  ### Project should be created in your wandb account\n",
    "            config  = config        ### Wandb Config for your run\n",
    "        )\n",
    "\n",
    "        ### Save your model architecture as a string with str(model)\n",
    "        model_arch  = str(model)\n",
    "\n",
    "        ### Save it in a txt file\n",
    "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "        file_write  = arch_file.write(model_arch)\n",
    "        arch_file.close()\n",
    "\n",
    "        ### Log it in your wandb run with wandb.save()\n",
    "        # wandb.save(\"model_arch.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/1047 [00:00<?, ?it/s]/tmp/ipykernel_193076/3134595652.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Train:   4%|▍         | 40/1047 [01:59<26:52,  1.60s/it, loss=4.5575, perplexity=116.3624]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(epoch+\u001b[32m1\u001b[39m, config[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     33\u001b[39m curr_lr = \u001b[38;5;28mfloat\u001b[39m(optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m train_loss, train_perplexity, attention_weights = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrain Loss \u001b[39m\u001b[38;5;132;01m{:.04f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m Train Perplexity \u001b[39m\u001b[38;5;132;01m{:.04f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m Learning Rate \u001b[39m\u001b[38;5;132;01m{:.04f}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m     38\u001b[39m     epoch + \u001b[32m1\u001b[39m, config[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m], train_loss, train_perplexity, curr_lr))\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (epoch >\u001b[32m0\u001b[39m )\u001b[38;5;129;01mand\u001b[39;00m (epoch % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m):    \u001b[38;5;66;03m# validate every 2 epochs to speed up training\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, optimizer)\u001b[39m\n\u001b[32m      7\u001b[39m running_loss        = \u001b[32m0.0\u001b[39m\n\u001b[32m      8\u001b[39m running_perplexity  = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_shifted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_golden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datagen/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datagen/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datagen/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1410\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1410\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1412\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datagen/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "e                   = 0\n",
    "best_loss           = 20\n",
    "\n",
    "checkpoint_root = os.path.join(os.getcwd(), \"checkpoints-basic-cnn-transformer\")\n",
    "os.makedirs(checkpoint_root, exist_ok=True)\n",
    "if USE_WANDB:\n",
    "    wandb.watch(model, log=\"all\")\n",
    "\n",
    "checkpoint_best_loss_model_filename     = 'checkpoint-best-loss-model.pth'\n",
    "checkpoint_last_epoch_filename          = 'checkpoint-epoch-'\n",
    "best_loss_model_path                    = os.path.join(checkpoint_root, checkpoint_best_loss_model_filename)\n",
    "\n",
    "if RESUME_LOGGING:\n",
    "    # change if you want to load best test model accordingly\n",
    "    checkpoint = torch.load(wandb.restore(checkpoint_best_loss_model_filename, run_path=\"\"+run_id).name)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    e = checkpoint['epoch']\n",
    "\n",
    "    print(\"Resuming from epoch {}\".format(e+1))\n",
    "    print(\"Epochs left: \", config['epochs']-e)\n",
    "    print(\"Optimizer: \\n\", optimizer)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "epochs = config[\"epochs\"]\n",
    "for epoch in range(e, epochs):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config[\"epochs\"]))\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    train_loss, train_perplexity, attention_weights = train_model(model, train_loader, optimizer)\n",
    "\n",
    "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Train Perplexity {:.04f}\\t Learning Rate {:.04f}\".format(\n",
    "        epoch + 1, config[\"epochs\"], train_loss, train_perplexity, curr_lr))\n",
    "\n",
    "    if (epoch >0 )and (epoch % 5 == 0):    # validate every 2 epochs to speed up training\n",
    "        levenshtein_distance = validate_fast(model, val_loader)\n",
    "        print(\"Levenshtein Distance {:.04f}\".format(levenshtein_distance))\n",
    "        if USE_WANDB:\n",
    "            wandb.log({\"train_loss\"     : train_loss,\n",
    "                    \"train_perplexity\"  : train_perplexity,\n",
    "                    \"learning_rate\"     : curr_lr,\n",
    "                    \"val_distance\"      : levenshtein_distance})\n",
    "\n",
    "    else:\n",
    "        if USE_WANDB:\n",
    "\n",
    "            wandb.log({\"train_loss\"     : train_loss,\n",
    "                    \"train_perplexity\"  : train_perplexity,\n",
    "                    \"learning_rate\"     : curr_lr})\n",
    "\n",
    "    # plotting the encoder-nearest and decoder-nearest attention weights\n",
    "    attention_keys = list(attention_weights.keys())\n",
    "\n",
    "    attention_weights_decoder_self       = attention_weights[attention_keys[0]][0].cpu().detach().numpy()\n",
    "    attention_weights_decoder_cross      = attention_weights[attention_keys[-1]][0].cpu().detach().numpy()\n",
    "\n",
    "    # saving the cross-attention weights\n",
    "    save_attention_plot(attention_weights_decoder_cross, epoch+100)\n",
    "\n",
    "    # plot_attention_weights((attention_weights[attention_keys[0]][0]).cpu().detach().numpy())\n",
    "    # plot_attention_weights(attention_weights[attention_keys[-1]][0].cpu().detach().numpy())\n",
    "\n",
    "    if config[\"scheduler\"] == \"ReduceLR\":\n",
    "        scheduler.step(levenshtein_distance)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    # ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
    "    # epoch_model_path = os.path.join(checkpoint_root, (checkpoint_last_epoch_filename + str(epoch) + '.pth'))\n",
    "    # save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, epoch_model_path)\n",
    "    ## wandb.save(epoch_model_path) ## Can't save on wandb for all epochs, may blow up storage\n",
    "\n",
    "\n",
    "    if best_loss >= train_loss:\n",
    "        best_loss = train_loss\n",
    "        save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, best_loss_model_path)\n",
    "        # wandb.save(best_loss_model_path)\n",
    "        print(\"Saved best training model\")\n",
    "\n",
    "### Finish your wandb run\n",
    "# run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
