{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset.dataloader import MyOcrDataloader\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import wandb\n",
    "import torch\n",
    "import gc\n",
    "import yaml\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from utils.utils import forward_pass_with_labels, update\n",
    "from jiwer import wer, cer\n",
    "from transformers import AutoFeatureExtractor, DeiTForImageClassificationWithTeacher\n",
    "from tqdm.auto import tqdm\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), '..')), \"config/main.yaml\")\n",
    "# with open(config_path, \"r\") as file:\n",
    "#     config = yaml.safe_load(file)\n",
    "\n",
    "# task  = config[\"TRAIN_TASK\"]\n",
    "# TRAIN_PATH =config[task+\"_\"+\"TRAIN_PATH\"]\n",
    "# VAL_PATH =config[task+\"_\"+\"VAL_PATH\"]\n",
    "# IMG_ROOT = config[task+\"_\"+\"IMG_ROOT\"]\n",
    "\n",
    "# MODEL_ID = config[\"MODEL_ID\"]\n",
    "# MODEL_ID = \"facebook/deit-base-distilled-patch16-384\"\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# # processor = TrOCRProcessor.from_pretrained(MODEL_ID)\n",
    "# processor = AutoFeatureExtractor.from_pretrained('facebook/deit-base-distilled-patch16-384')\n",
    "# # model =VisionEncoderDecoderModel.from_pretrained(MODEL_ID).to(device)\n",
    "# model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "#     \"facebook/deit-base-distilled-patch16-384\", \"rasyosef/bert-medium-amharic\", from_tf =True, cache_dir = \"/home/ubuntu/data/cache_dir\"\n",
    "# ).to(device)\n",
    "# model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
    "# model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# model.config.vocab_size = model.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), '..')), \"config/main.yaml\")\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "task  = config[\"TRAIN_TASK\"]\n",
    "TRAIN_PATH =config[task+\"_\"+\"TRAIN_PATH\"]\n",
    "VAL_PATH =config[task+\"_\"+\"VAL_PATH\"]\n",
    "IMG_ROOT = config[task+\"_\"+\"IMG_ROOT\"]\n",
    "\n",
    "MODEL_ID = config[\"MODEL_ID\"]\n",
    "MODEL_ID = \"facebook/deit-base-distilled-patch16-384\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# processor = TrOCRProcessor.from_pretrained(MODEL_ID)\n",
    "processor = AutoFeatureExtractor.from_pretrained('facebook/deit-base-distilled-patch16-384')\n",
    "# model =VisionEncoderDecoderModel.from_pretrained(MODEL_ID).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rasyosef/bert-medium-amharic\")\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"facebook/deit-base-distilled-patch16-384\", \"rasyosef/bert-medium-amharic\", from_tf =True, cache_dir = \"/home/ubuntu/data/cache_dir\"\n",
    ").to(device)\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.decoder.config.vocab_size\n",
    "model.generation_config.decoder_start_token_id = tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check for data loader\n",
    "os.chdir(\"../dataset\")\n",
    "train_data = MyOcrDataloader(TRAIN_PATH, preprocessor=processor, tokenizer=tokenizer, img_root=IMG_ROOT)\n",
    "val_data = MyOcrDataloader(VAL_PATH, preprocessor=processor, tokenizer  = tokenizer, img_root=IMG_ROOT)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    batch_size  = config['BATCH_SIZE'],\n",
    "    shuffle     = True,\n",
    "    collate_fn= val_data.collate_fn\n",
    "    )\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    batch_size  = config['BATCH_SIZE'],\n",
    "\n",
    "    shuffle     = False,\n",
    "    collate_fn= train_data.collate_fn\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[0].shape, batch[1].shape,tokenizer.batch_decode(batch[1], skip_special_tokens=True))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     # print(batch[0].shape, batch[1].shape,)\n",
    "#     labels = tokenizer.batch_decode(batch[1], skip_special_tokens=True)\n",
    "\n",
    "   \n",
    "\n",
    "#     # Generate text\n",
    "#     generated_ids = model.generate(batch[0].cuda())\n",
    "\n",
    "#     # Decode the generated text\n",
    "#     generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "#     print(generated_text, labels[0])  \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Fetch a single batch\n",
    "# train_wers = 0\n",
    "# train_cers = 0\n",
    "# samples = 0\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     # Get the images and labels\n",
    "#     images = batch[0]\n",
    "#     labels = tokenizer.batch_decode(batch[1], skip_special_tokens=True)\n",
    "\n",
    "#     # Move images to GPU (if available)\n",
    "#     images = images.cuda()\n",
    "\n",
    "#     # Generate text using the model\n",
    "#     generated_ids = model.generate(images)\n",
    "#     generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "#     for pred, actual in zip(generated_texts, labels):\n",
    "#         train_wers += wer(actual, pred)\n",
    "#         train_cers += cer(actual, pred)\n",
    "\n",
    "#     samples += len(generated_texts)\n",
    "#     # Plot the images and corresponding texts\n",
    "#     fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "#     print(train_wers, train_cers)\n",
    "#     if len(images) == 1:\n",
    "#         axes = [axes]  # Ensure axes is iterable even for a single image\n",
    "\n",
    "#     for idx, (image, gt_text, pred_text) in enumerate(zip(images, labels, generated_texts)):\n",
    "#         # Convert tensor to numpy and permute dimensions for plotting\n",
    "#         image = image.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "#         # Display the image\n",
    "#         axes[idx].imshow(image)\n",
    "#         axes[idx].set_title(f\"GT: {gt_text}\\nPred: {pred_text}\")\n",
    "#         axes[idx].axis('off')  # Hide axes\n",
    "#     break\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_train_epoch(model, train_data_loader, scaler, optimizer):\n",
    "    model.train()\n",
    "    pbar = tqdm(range(len(train_data_loader)))\n",
    "    train_loss= 0.0\n",
    "    \n",
    "    for i,batch in enumerate(train_data_loader):\n",
    "        loss, logits = update(model, scaler, optimizer, batch)\n",
    "        preds = tokenizer.batch_decode(logits.argmax(dim=2), skip_special_tokens=True)\n",
    "        actuals = tokenizer.batch_decode(batch[1], skip_special_tokens=True)\n",
    "\n",
    "        train_loss += (1/(i+1))*(loss-train_loss)\n",
    "        perplexity          = torch.exp(torch.tensor(train_loss))\n",
    "        # print(logits.shape,\"preds\", preds, \"\\n\", \"=\"*20, \"\\nactuals\", actuals)\n",
    "        # for pred, actual in zip(preds, actuals):\n",
    "        #     train_wers += wer(actual, pred)\n",
    "        #     train_cers += cer(actual, pred)\n",
    "\n",
    "   \n",
    "        pbar.set_description(f\"Train Loss: {train_loss:.4f} Perplexity: {perplexity:.4f}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return train_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation (model, val_data_loader):\n",
    "    model.eval()\n",
    "    pbar = tqdm(range(len(val_data_loader)))\n",
    "    running_distance = 0.0\n",
    "    running_cer = 0.0\n",
    "    running_wer = 0.0\n",
    "    running_char_f1 = 0.0\n",
    "    running_word_f1 =0.0\n",
    "    samples = 0.0\n",
    "    for i,batch in enumerate(val_data_loader):\n",
    "        images = batch[0].to(model.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            generated_ids = model.generate(images)\n",
    "            \n",
    "        generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        actuals = tokenizer.batch_decode(batch[1], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "        # print(logits.shape,\"preds\", preds, \"\\n\", \"=\"*20, \"\\nactuals\", actuals)\n",
    "        # for pred, actual in zip(preds, actuals):\n",
    "        #     train_wers += wer(actual, pred)\n",
    "        #     train_cers += cer(actual, pred)\n",
    "        dist, cer, wer, charf1, word_f1 = compute_metrics(generated_texts, actuals)\n",
    "        running_distance += dist\n",
    "        running_cer += cer\n",
    "        running_wer += wer\n",
    "        running_char_f1 += charf1\n",
    "        running_word_f1 += word_f1\n",
    "        samples += len(generated_ids)\n",
    "   \n",
    "        pbar.set_description(f\"Val WER: {running_wer/samples:.4f} CER: {running_cer/samples:.4f} LevDistance: {running_distance/samples:.4f}  Charf1: {running_char_f1/samples:.4f} word_f1: {running_word_f1/samples:.4f}\")\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(\"pred\", generated_texts[:2])\n",
    "    print(\"actual\", actuals[:2])\n",
    "    pbar.close()\n",
    "    return running_distance/samples, running_cer/samples, running_wer/samples, running_char_f1/samples, running_word_f1/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, tdataloader,vdataloader, optimizer, scaler, epochs=3, log_dir=\"logs/a\"):\n",
    "#     os.makedirs(log_dir, exist_ok=True)\n",
    "#     with open(f\"{log_dir}/train.txt\", \"w\") as tf, open(f\"{log_dir}/val.txt\", \"w\") as vf:\n",
    "#         tf.write(f\"Epoch,Loss,Score\\n\")\n",
    "#         vf.write(\"Epoch,Loss,Score\\n\")\n",
    "#     best_val_score = 0    \n",
    "#     train_loss = 0\n",
    "#     for epoch in range(1, 1+epochs):\n",
    "#         loss, perplexity  = one_train_epoch(model, tdataloader, optimizer, scaler )\n",
    "#         levenshtein_distance, cer, wer, charf1, wordf1 = validation(model, val_loader)\n",
    "#         # with open(f\"{log_dir}/train.txt\", \"a+\") as tf, open(f\"{log_dir}/val.txt\", \"a+\") as vf:\n",
    "#         #     tf.write(f\"{epoch},{train_loss},{tscore}\\n\")\n",
    "#         #     vf.write(f\"{epoch},{val_loss},{score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), 1e-4)\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scaler =  torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "# train(model, train_loader, val_loader,optimizer, scaler, epochs=15,log_dir=\"logs/vit_amharic_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' defining optimizer '''\n",
    "# vocab_size = len(tokenizer)\n",
    "# weights = torch.ones(vocab_size).to(\"cuda\")  # default weight = 1 for all\n",
    "# whitespace_token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' ')[0])\n",
    "# # Decrease weight for whitespace token\n",
    "# weights[whitespace_token_id] = 0.1  # e.g., reduce impact by 90%\n",
    "loss_func   = torch.nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "scaler      = torch.cuda.amp.GradScaler()\n",
    "if config[\"optimizer\"] == \"SGD\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.SGD(model.parameters(),\n",
    "                              lr=config[\"learning_rate\"],\n",
    "                              momentum=config[\"momentum\"],\n",
    "                              weight_decay=1E-4,\n",
    "                              nesterov=config[\"nesterov\"])\n",
    "\n",
    "elif config[\"optimizer\"] == \"Adam\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.Adam(model.parameters(),\n",
    "                               lr=float(config[\"learning_rate\"]),\n",
    "                               weight_decay=1e-4)\n",
    "\n",
    "elif config[\"optimizer\"] == \"AdamW\":\n",
    "  # feel free to change any of the initializations you like to fit your needs\n",
    "  optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                lr=float(config[\"learning_rate\"]),\n",
    "                                weight_decay=0.01)\n",
    "\n",
    "''' defining scheduler '''\n",
    "\n",
    "if config[\"scheduler\"] == \"ReduceLR\":\n",
    "  #Feel Free to change any of the initializations you like to fit your needs\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                factor=config[\"factor\"], patience=config[\"patience\"], min_lr=1E-8, verbose=True)\n",
    "\n",
    "elif config[\"scheduler\"] == \"CosineAnnealing\":\n",
    "  #Feel Free to change any of the initializations you like to fit your needs\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                T_max = 35, eta_min=1E-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using WandB? resume training?\n",
    "\n",
    "USE_WANDB = config[\"USE_WANDB\"]\n",
    "RESUME_LOGGING = False\n",
    "\n",
    "# creating your WandB run\n",
    "run_name = \"{}_Transformer_ENC-{}/{}_DEC-{}/{}_{}_{}_{}_{}\".format(\n",
    "    config[\"Name\"],\n",
    "    config[\"enc_num_layers\"],       # only used in Part II with the Transformer Encoder\n",
    "    config[\"enc_num_heads\"],        # only used in Part II with the Transformer Encoder\n",
    "    config[\"dec_num_layers\"],\n",
    "    config[\"dec_num_heads\"],\n",
    "    config[\"d_model\"],\n",
    "    config[\"d_ff\"],\n",
    "    config[\"optimizer\"],\n",
    "    config[\"scheduler\"])\n",
    "task = \"handonly\"\n",
    "\n",
    "if USE_WANDB:\n",
    "\n",
    "    wandb.login(key=\"3c7b273814544590b64c54d9a5242bde38616e02\", relogin=True) # TODO enter your key here\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = \"\"\n",
    "        run = wandb.init(\n",
    "            id     = run_id,        ### Insert specific run id here if you want to resume a previous run\n",
    "            resume = True,          ### You need this to resume previous runs, but comment out reinit=True when using this\n",
    "            project = task+\"ocr-cnn-lstm\",  ### Project should be created in your wandb account\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name    = run_name,     ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            reinit  = True,         ### Allows reinitalizing runs when you re-run this cell\n",
    "            project = task+\"ocr-cnn-lstm\",  ### Project should be created in your wandb account\n",
    "            config  = config        ### Wandb Config for your run\n",
    "        )\n",
    "\n",
    "        ### Save your model architecture as a string with str(model)\n",
    "        model_arch  = str(model)\n",
    "\n",
    "        ### Save it in a txt file\n",
    "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "        file_write  = arch_file.write(model_arch)\n",
    "        arch_file.close()\n",
    "\n",
    "        ### Log it in your wandb run with wandb.save()\n",
    "        # wandb.save(\"model_arch.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val WER: 0.9411 CER: 0.8047 LevDistance: 60.9733  Charf1: 0.0593 word_f1: 0.0192:  22%|██▏       | 34/157 [03:52<14:02,  6.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2529 Perplexity: 1.2878: 100%|██████████| 157/157 [03:53<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50: \n",
      "Train Loss 0.2529\t Train Perplexity 1.2878\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2391 Perplexity: 1.2701: 100%|██████████| 157/157 [03:51<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50: \n",
      "Train Loss 0.2391\t Train Perplexity 1.2701\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2378 Perplexity: 1.2685: 100%|██████████| 157/157 [03:53<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50: \n",
      "Train Loss 0.2378\t Train Perplexity 1.2685\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2347 Perplexity: 1.2645: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50: \n",
      "Train Loss 0.2347\t Train Perplexity 1.2645\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2027 Perplexity: 1.2246: 100%|██████████| 157/157 [03:51<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50: \n",
      "Train Loss 0.2027\t Train Perplexity 1.2246\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1837 Perplexity: 1.2016: 100%|██████████| 157/157 [03:56<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50: \n",
      "Train Loss 0.1837\t Train Perplexity 1.2016\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1716 Perplexity: 1.1872: 100%|██████████| 157/157 [03:55<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50: \n",
      "Train Loss 0.1716\t Train Perplexity 1.1872\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1629 Perplexity: 1.1770: 100%|██████████| 157/157 [03:51<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50: \n",
      "Train Loss 0.1629\t Train Perplexity 1.1770\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1546 Perplexity: 1.1672: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50: \n",
      "Train Loss 0.1546\t Train Perplexity 1.1672\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1464 Perplexity: 1.1577: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50: \n",
      "Train Loss 0.1464\t Train Perplexity 1.1577\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1399 Perplexity: 1.1502: 100%|██████████| 157/157 [03:53<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50: \n",
      "Train Loss 0.1399\t Train Perplexity 1.1502\t Learning Rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val WER: 0.9436 CER: 0.8082 LevDistance: 61.3860  Charf1: 0.0603 word_f1: 0.0176: 100%|██████████| 157/157 [04:50<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred ['ዕውቅናጡ ኦባማ ጥሎሄድባውያን እንደሚጣ በግልጽሯል ኦባማ ውን ከቤተሰቡ ለፊትኩ ወደነትቀና', 'ወደ ናቸው :']\n",
      "actual ['አትታለል አማራው አልበደለህም አማራው የተቋቋመውን አውሮፓዊ እና ዝርያውን አምላክ አድርጎ ሊያጠፋህ በገንዘብ ተገዝቶ በብሔር እየከፋፈልህ ያለው የብሔር ነፃ አዎጭው እንጂ', 'ማሳያ ሥራ ማስጀመር : :']\n",
      "Saved distance training model\n",
      "Levenshtein Distance 61.3860\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1315 Perplexity: 1.1405: 100%|██████████| 157/157 [03:55<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50: \n",
      "Train Loss 0.1315\t Train Perplexity 1.1405\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1260 Perplexity: 1.1343: 100%|██████████| 157/157 [03:53<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50: \n",
      "Train Loss 0.1260\t Train Perplexity 1.1343\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1209 Perplexity: 1.1285: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50: \n",
      "Train Loss 0.1209\t Train Perplexity 1.1285\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1186 Perplexity: 1.1260: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50: \n",
      "Train Loss 0.1186\t Train Perplexity 1.1260\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1138 Perplexity: 1.1205: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50: \n",
      "Train Loss 0.1138\t Train Perplexity 1.1205\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1073 Perplexity: 1.1133: 100%|██████████| 157/157 [03:53<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50: \n",
      "Train Loss 0.1073\t Train Perplexity 1.1133\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1008 Perplexity: 1.1060: 100%|██████████| 157/157 [03:56<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50: \n",
      "Train Loss 0.1008\t Train Perplexity 1.1060\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0988 Perplexity: 1.1039: 100%|██████████| 157/157 [03:54<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50: \n",
      "Train Loss 0.0988\t Train Perplexity 1.1039\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0952 Perplexity: 1.0998: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50: \n",
      "Train Loss 0.0952\t Train Perplexity 1.0998\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0919 Perplexity: 1.0962: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50: \n",
      "Train Loss 0.0919\t Train Perplexity 1.0962\t Learning Rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val WER: 0.9430 CER: 0.8035 LevDistance: 61.2323  Charf1: 0.0605 word_f1: 0.0192: 100%|██████████| 157/157 [04:48<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred ['መንፈስፈንአጦስ ሊያስ በሚችል መሰናክል ከዘ በኋላን ቤተ ከጥፋት መንግሥ ሰማይሳድ በኋላ ራንእ', 'ወደ ናቸው :']\n",
      "actual ['አትታለል አማራው አልበደለህም አማራው የተቋቋመውን አውሮፓዊ እና ዝርያውን አምላክ አድርጎ ሊያጠፋህ በገንዘብ ተገዝቶ በብሔር እየከፋፈልህ ያለው የብሔር ነፃ አዎጭው እንጂ', 'ማሳያ ሥራ ማስጀመር : :']\n",
      "Levenshtein Distance 61.2323\n",
      "\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0883 Perplexity: 1.0923: 100%|██████████| 157/157 [03:53<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50: \n",
      "Train Loss 0.0883\t Train Perplexity 1.0923\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0842 Perplexity: 1.0879: 100%|██████████| 157/157 [03:53<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50: \n",
      "Train Loss 0.0842\t Train Perplexity 1.0879\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0812 Perplexity: 1.0845: 100%|██████████| 157/157 [03:56<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50: \n",
      "Train Loss 0.0812\t Train Perplexity 1.0845\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0811 Perplexity: 1.0844: 100%|██████████| 157/157 [03:53<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50: \n",
      "Train Loss 0.0811\t Train Perplexity 1.0844\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0797 Perplexity: 1.0830: 100%|██████████| 157/157 [03:56<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50: \n",
      "Train Loss 0.0797\t Train Perplexity 1.0830\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0773 Perplexity: 1.0804: 100%|██████████| 157/157 [03:53<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50: \n",
      "Train Loss 0.0773\t Train Perplexity 1.0804\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0724 Perplexity: 1.0751: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50: \n",
      "Train Loss 0.0724\t Train Perplexity 1.0751\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0707 Perplexity: 1.0732: 100%|██████████| 157/157 [04:01<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50: \n",
      "Train Loss 0.0707\t Train Perplexity 1.0732\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0694 Perplexity: 1.0718: 100%|██████████| 157/157 [03:53<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50: \n",
      "Train Loss 0.0694\t Train Perplexity 1.0718\t Learning Rate 0.0001\n",
      "\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0699 Perplexity: 1.0724:  41%|████▏     | 65/157 [01:37<02:14,  1.46s/it]"
     ]
    }
   ],
   "source": [
    "e                   = 0\n",
    "best_loss           = 0\n",
    "best_distance  = 0\n",
    "checkpoint_root = os.path.join(os.getcwd(), \"checkpoints-basic-cnn-transformer\")\n",
    "os.makedirs(checkpoint_root, exist_ok=True)\n",
    "\n",
    "USE_WANDB = config[\"USE_WANDB\"]\n",
    "if USE_WANDB:\n",
    "    wandb.watch(model, log=\"all\")\n",
    "task =  config[\"TRAIN_TASK\"]\n",
    "checkpoint_best_loss_model_filename     = task +'checkpoint-best-loss-model.pth' \n",
    "checkpoint_best_distance_model_filename     = task +'checkpoint-best-distance-model.pth'\n",
    "\n",
    "checkpoint_last_epoch_filename          = 'checkpoint-epoch-'\n",
    "best_loss_model_path                    = os.path.join(checkpoint_root, checkpoint_best_loss_model_filename)\n",
    "best_distance_model_path                    = os.path.join(checkpoint_root, checkpoint_best_distance_model_filename)\n",
    "\n",
    "\n",
    "if RESUME_LOGGING:\n",
    "    # change if you want to load best test model accordingly\n",
    "    checkpoint = torch.load(wandb.restore(checkpoint_best_loss_model_filename, run_path=\"\"+run_id).name)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    e = checkpoint['epoch']\n",
    "\n",
    "    print(\"Resuming from epoch {}\".format(e+1))\n",
    "    print(\"Epochs left: \", config['epochs']-e)\n",
    "    print(\"Optimizer: \\n\", optimizer)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "epochs = config[\"epochs\"]\n",
    "for epoch in range(e, epochs):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config[\"epochs\"]))\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    train_loss, train_perplexity = one_train_epoch(model, train_loader, optimizer, scaler )\n",
    "\n",
    "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Train Perplexity {:.04f}\\t Learning Rate {:.04f}\".format(\n",
    "        epoch + 1, config[\"epochs\"], train_loss, train_perplexity, curr_lr))\n",
    "\n",
    "    if (epoch >1 )and (epoch % 10 == 0):    # validate every 2 epochs to speed up training\n",
    "        levenshtein_distance, cer, wer, charf1, wordf1 = validation(model, val_loader)\n",
    "        if best_distance <= cer:\n",
    "            best_distance = cer\n",
    "            save_model(model, optimizer, scheduler, ['val_distance', cer], epoch, best_distance_model_path)\n",
    "            # wandb.save(best_loss_model_path)\n",
    "            print(\"Saved distance training model\")\n",
    "        print(\"Levenshtein Distance {:.04f}\".format(levenshtein_distance))\n",
    "        if USE_WANDB:\n",
    "            wandb.log({\"train_loss\"     : train_loss,\n",
    "                    \"train_perplexity\"  : train_perplexity,\n",
    "                    \"learning_rate\"     : curr_lr,\n",
    "                    \"val_distance\"      : levenshtein_distance,\n",
    "                    \"charf1\": charf1,\n",
    "                    \"wordf1\": wordf1,})\n",
    "\n",
    "    else:\n",
    "        if USE_WANDB:\n",
    "\n",
    "            wandb.log({\"train_loss\"     : train_loss,\n",
    "                    \"train_perplexity\"  : train_perplexity,\n",
    "                    \"learning_rate\"     : curr_lr})\n",
    "\n",
    "    if best_loss >= train_loss:\n",
    "        best_loss = train_loss\n",
    "        save_model(model, optimizer, scheduler, ['train_loss', train_loss], epoch, best_loss_model_path)\n",
    "        # wandb.save(best_loss_model_path)\n",
    "        print(\"Saved best training model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
